{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"y415xgE_VkV2"},"source":["# What is this Notebook?"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"qvjIJhXqVdo6"},"source":["Hallo Leute,\n","\n","das ist der Versuch das Conv. Neuronale Netz (CNN) vom *Filter Network* des *Waggle Dance Detectors* zum Laufen zu bringen.\n","\n","**Source code**: [GitHub: BioroboticsLab/bb_wdd_filter](https://github.com/BioroboticsLab/bb_wdd_filter)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"cogE5qn7SpZy"},"source":["# Implementation 02 - Clone from GitHub"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"qbXcCCYGUssR"},"source":["### Train Model"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33261,"status":"ok","timestamp":1683540080651,"user":{"displayName":"IT Joe","userId":"14568903722424047290"},"user_tz":-120},"id":"GMDHkoA-m61R","outputId":"03f64dd4-e3de-4f0b-cf51-c3285e64f0c0"},"outputs":[{"name":"stderr","output_type":"stream","text":["/srv/data/joeh97/anaconda3/envs/uni-morty/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["#%pip install git+https://github.com/linusb20/bb_wdd_filter.git\n","import bb_wdd_filter\n","import wandb"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2156,"status":"ok","timestamp":1683542176104,"user":{"displayName":"IT Joe","userId":"14568903722424047290"},"user_tz":-120},"id":"vcri3xrBm-br","outputId":"0fa73637-3659-4ab3-8ab8-78bed91ad6f9"},"outputs":[{"name":"stdout","output_type":"stream","text":["bb-wdd-filter                 0.1        /srv/data/joeh97/github/bb_wdd_filter\n","wandb                         0.15.2\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip list |  grep -E 'bb-wdd-filter|wandb'"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":239,"status":"ok","timestamp":1683541636020,"user":{"displayName":"IT Joe","userId":"14568903722424047290"},"user_tz":-120},"id":"E6WFsITkv50P"},"outputs":[],"source":["import argparse\n","\n","import pickle\n","import numpy as np\n","import os\n","import torch.nn\n","\n","import bb_wdd_filter.dataset\n","import bb_wdd_filter.models_supervised\n","import bb_wdd_filter.trainer_supervised\n","import bb_wdd_filter.visualization\n","\n","\n","def run_wdd(\n","    gt_data_path,\n","    num_workers = 0,\n","    checkpoint_path=None,\n","    continue_training=True,\n","    epochs=1000,\n","    remap_wdd_dir=None,\n","    image_size=32,\n","    images_in_archives=True,\n","    multi_gpu=False,\n","    image_scale=0.5,\n","    batch_size=\"auto\",\n","    max_lr=0.002 * 8,\n","    wandb_entity=None,\n","    wandb_project=\"wdd-image-classification\",\n","):\n","    \"\"\"\n","    Arguments:\n","        gt_data_path (string)\n","            Path to the .pickle file containing the ground-truth labels and paths.\n","        remap_wdd_dir (string, optional)\n","            Prefix of the path where the image data is saved. The paths in gt_data_path\n","            will be changed to point to this directory instead.\n","        images_in_archives (bool)\n","            Whether the images of the single waggle frames are saved withing an images.zip\n","            file in each WDD subdirectory.\n","        checkpoint_path (string, optional)\n","            Filename to which the model will be saved regularly during training.\n","            The model will be saved on every epoch AND every X batches.\n","        continue_training (bool)\n","            Whether to try to continue training from last checkpoint. Will use the same\n","            wandb run ID. Auto set to \"false\" in case no checkpoint is found.\n","        epochs (int)\n","            Number of epochs to train for.\n","            As the model is saved after every epoch in 'checkpoint_path' and as the logs are\n","            streamed live to wandb.ai, it's save to interrupt the training after any epoch.\n","        image_size (int)\n","            Width and height of images that are passed to the model.\n","        image_scale (float)\n","            Scale factor for the data. E.g. 0.5 will scale the images to half resolution.\n","            That allows for a wider FoV for the model by sacrificing some resolution.\n","        max_lr (float)\n","            The training uses a learning rate scheduler (OneCycleLR) for each epoch\n","            where max_lr constitutes the peak learning rate.\n","        wandb_entity (string, optional)\n","            User name for wandb.ai that the training will log data to.\n","        wandb_project (string)\n","            Project name for wandb.ai.\n","\n","    \"\"\"\n","\n","    with open(gt_data_path, \"rb\") as f:\n","        wdd_gt_data = pickle.load(f)\n","        gt_data_df = [(key,) + v for key, v in wdd_gt_data.items()]\n","\n","    all_indices = np.arange(len(gt_data_df))\n","    test_indices = all_indices[::10]\n","    train_indices = [idx for idx in all_indices if not (idx in test_indices)]\n","\n","    print(\"Train set:\")\n","    dataset = bb_wdd_filter.dataset.SupervisedDataset(\n","        [gt_data_df[idx] for idx in train_indices],\n","        images_in_archives=images_in_archives,\n","        image_size=image_size,\n","        load_wdd_vectors=True,\n","        load_wdd_durations=True,\n","        remap_paths_to=remap_wdd_dir,\n","    )\n","\n","    print(\"Test set:\")\n","    # The evaluator's job is to regularly evaluate the training progress on the test dataset.\n","    # It will calculate additional statistics that are logged over the wandb connection.\n","    evaluator = bb_wdd_filter.dataset.SupervisedValidationDatasetEvaluator(\n","        [gt_data_df[idx] for idx in test_indices],\n","        images_in_archives=images_in_archives,\n","        image_size=image_size,\n","        remap_paths_to=remap_wdd_dir,\n","        default_image_scale=image_scale,\n","    )\n","\n","    model = bb_wdd_filter.models_supervised.WDDClassificationModel(\n","        image_size=image_size\n","    )\n","\n","    if multi_gpu:\n","        model = torch.nn.DataParallel(model)\n","\n","    model = model.cuda()\n","\n","    if batch_size == \"auto\":\n","        # The batch size here is calculated so that it fits on two RTX 2080 Ti in multi-GPU mode.\n","        # Note that a smaller batch size might also need a smaller learning rate.\n","        factor = 1\n","        if multi_gpu:\n","            factor = 2\n","        batch_size = int((64 * 7 * factor) / ((image_size * image_size) / (32 * 32)))\n","    else:\n","        batch_size = int(batch_size)\n","\n","    print(\n","        \"N pars: \",\n","        str(sum(p.numel() for p in model.parameters() if p.requires_grad)),\n","        \"batch size: \",\n","        batch_size,\n","    )\n","\n","    wandb_config = None\n","    if False:\n","        # Project name is fixed so far.\n","        # This provides a logging interface to wandb.ai.\n","        wandb_config = (dict(project=wandb_project, entity=wandb_entity),)\n","\n","    trainer = bb_wdd_filter.trainer_supervised.SupervisedTrainer(\n","        dataset,\n","        model,\n","        wandb_config=dict(),\n","        save_path=checkpoint_path,\n","        batch_size=batch_size,\n","        num_workers=num_workers,\n","        continue_training=continue_training,\n","        image_size=image_size,\n","        batch_sampler_kwargs=dict(\n","            image_scale_factor=image_scale,\n","            inflate_dataset_factor=1000,\n","            augmentation_per_image=False,\n","        ),\n","        test_set_evaluator=evaluator,\n","        eval_test_set_every_n_samples=2000,\n","        save_every_n_samples=200000,\n","        max_lr=max_lr,\n","        batches_to_reach_maximum_augmentation=1000,\n","    )\n","\n","    trainer.run_epochs(epochs)\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":684,"referenced_widgets":["5973c46b83b14118a0d73c52f49467a6","ea8f07dc73514c3ca54a31221587325c","ab59a22a2f894e18a79c5a5418003716","dfdd2fb7295b46b2842a79c9340fd28e","0dc4453a7a0f439aa97bfa86ac94f464","7bca261d5a534ed1805f89985975c0e0","693f4e338ac94244afdebff738d77bd1","13ac7b6f0f4a4e65962f14e14be9c9a3","c33d2c451f434ee6a30f52837b647ff6","45001d3bcd7345ef97da5067ffda7329","77b87fb1739741b7849f2984d7afdc5c"]},"executionInfo":{"elapsed":134810,"status":"error","timestamp":1683542321997,"user":{"displayName":"IT Joe","userId":"14568903722424047290"},"user_tz":-120},"id":"e7vTFG5lxbEZ","outputId":"9e21938a-29be-4cf8-bad6-c27b6d72d210"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train set:\n","Found 908 waggle folders.\n","Test set:\n","Found 101 waggle folders.\n","N pars:  121431 batch size:  448\n","SupervisedTrainer:init 1\n","SupervisedTrainer:init 2\n","Hello 1\n","Hello 2\n","1\n","3\n","4\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/2026 [00:00<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["5\n","6\n"]},{"name":"stderr","output_type":"stream","text":["                                        \r"]},{"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 1.51 GiB (GPU 0; 11.91 GiB total capacity; 1.58 GiB already allocated; 802.00 MiB free; 1.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m run_wdd(\n\u001b[1;32m      2\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m      3\u001b[0m     num_workers\u001b[39m=\u001b[39;49m\u001b[39m6\u001b[39;49m,\n\u001b[1;32m      4\u001b[0m     continue_training\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m      5\u001b[0m     gt_data_path\u001b[39m=\u001b[39;49m    \u001b[39m\"\u001b[39;49m\u001b[39m../../../data/wdd_ground_truth/ground_truth_wdd_angles.pickle\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      6\u001b[0m     remap_wdd_dir\u001b[39m=\u001b[39;49m      \u001b[39m\"\u001b[39;49m\u001b[39m../../../data/wdd_ground_truth/\u001b[39;49m\u001b[39m\"\u001b[39;49m ,\n\u001b[1;32m      7\u001b[0m     checkpoint_path\u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m./wdd_filtering_supervised_model.pt\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      8\u001b[0m     images_in_archives\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m      9\u001b[0m )\n","Cell \u001b[0;32mIn[3], line 147\u001b[0m, in \u001b[0;36mrun_wdd\u001b[0;34m(gt_data_path, num_workers, checkpoint_path, continue_training, epochs, remap_wdd_dir, image_size, images_in_archives, multi_gpu, image_scale, batch_size, max_lr, wandb_entity, wandb_project)\u001b[0m\n\u001b[1;32m    124\u001b[0m     wandb_config \u001b[39m=\u001b[39m (\u001b[39mdict\u001b[39m(project\u001b[39m=\u001b[39mwandb_project, entity\u001b[39m=\u001b[39mwandb_entity),)\n\u001b[1;32m    126\u001b[0m trainer \u001b[39m=\u001b[39m bb_wdd_filter\u001b[39m.\u001b[39mtrainer_supervised\u001b[39m.\u001b[39mSupervisedTrainer(\n\u001b[1;32m    127\u001b[0m     dataset,\n\u001b[1;32m    128\u001b[0m     model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    144\u001b[0m     batches_to_reach_maximum_augmentation\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m,\n\u001b[1;32m    145\u001b[0m )\n\u001b[0;32m--> 147\u001b[0m trainer\u001b[39m.\u001b[39;49mrun_epochs(epochs)\n","File \u001b[0;32m/srv/data/joeh97/github/bb_wdd_filter/bb_wdd_filter/trainer.py:279\u001b[0m, in \u001b[0;36mTrainer.run_epochs\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n):\n\u001b[1;32m    277\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mHello 2\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 279\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_epoch()\n\u001b[1;32m    280\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mHello 3\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    282\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal_epochs \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n","File \u001b[0;32m/srv/data/joeh97/github/bb_wdd_filter/bb_wdd_filter/trainer.py:228\u001b[0m, in \u001b[0;36mTrainer.run_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_scale_augmenters()\n\u001b[1;32m    226\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m6\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 228\u001b[0m loss_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_batch(\u001b[39m*\u001b[39;49mbatch)\n\u001b[1;32m    229\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m7\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    231\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n","File \u001b[0;32m/srv/data/joeh97/github/bb_wdd_filter/bb_wdd_filter/trainer.py:131\u001b[0m, in \u001b[0;36mTrainer.run_batch\u001b[0;34m(self, images, vectors, durations, labels)\u001b[0m\n\u001b[1;32m    129\u001b[0m     losses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_batch_fn(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, images, vectors, durations, labels)\n\u001b[1;32m    130\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 131\u001b[0m     losses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mrun_batch(images, vectors, durations, labels)\n\u001b[1;32m    133\u001b[0m total_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m    134\u001b[0m \u001b[39mfor\u001b[39;00m loss_name, value \u001b[39min\u001b[39;00m losses\u001b[39m.\u001b[39mitems():\n","File \u001b[0;32m/srv/data/joeh97/github/bb_wdd_filter/bb_wdd_filter/models_supervised.py:242\u001b[0m, in \u001b[0;36mSupervisedModelTrainWrapper.run_batch\u001b[0;34m(self, images, vectors, durations, labels)\u001b[0m\n\u001b[1;32m    239\u001b[0m batch_size, temp_dimension \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mshape[:\u001b[39m2\u001b[39m]\n\u001b[1;32m    240\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[0;32m--> 242\u001b[0m all_outputs \u001b[39m=\u001b[39m model(images)\n\u001b[1;32m    243\u001b[0m classes_hat, vectors_hat, durations_hat, _ \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpostprocess_predictions(all_outputs, return_raw\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)        \n\u001b[1;32m    245\u001b[0m losses \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m()\n","File \u001b[0;32m/srv/data/joeh97/anaconda3/envs/uni-morty/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m/srv/data/joeh97/github/bb_wdd_filter/bb_wdd_filter/models_supervised.py:134\u001b[0m, in \u001b[0;36mWDDClassificationModel.forward\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n\u001b[1;32m    133\u001b[0m     images\u001b[39m.\u001b[39mrequires_grad \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mcheckpoint\u001b[39m.\u001b[39;49mcheckpoint_sequential(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseq, \u001b[39m4\u001b[39;49m, images)\n\u001b[1;32m    135\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseq(images)\n","File \u001b[0;32m/srv/data/joeh97/anaconda3/envs/uni-morty/lib/python3.11/site-packages/torch/utils/checkpoint.py:330\u001b[0m, in \u001b[0;36mcheckpoint_sequential\u001b[0;34m(functions, segments, input, use_reentrant, **kwargs)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[39mfor\u001b[39;00m start \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, segment_size \u001b[39m*\u001b[39m (segments \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m), segment_size):\n\u001b[1;32m    329\u001b[0m     end \u001b[39m=\u001b[39m start \u001b[39m+\u001b[39m segment_size \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 330\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m checkpoint(\n\u001b[1;32m    331\u001b[0m         run_function(start, end, functions),\n\u001b[1;32m    332\u001b[0m         \u001b[39minput\u001b[39;49m,\n\u001b[1;32m    333\u001b[0m         use_reentrant\u001b[39m=\u001b[39;49muse_reentrant,\n\u001b[1;32m    334\u001b[0m         preserve_rng_state\u001b[39m=\u001b[39;49mpreserve\n\u001b[1;32m    335\u001b[0m     )\n\u001b[1;32m    336\u001b[0m \u001b[39mreturn\u001b[39;00m run_function(end \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(functions) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m, functions)(\u001b[39minput\u001b[39m)\n","File \u001b[0;32m/srv/data/joeh97/anaconda3/envs/uni-morty/lib/python3.11/site-packages/torch/utils/checkpoint.py:249\u001b[0m, in \u001b[0;36mcheckpoint\u001b[0;34m(function, use_reentrant, *args, **kwargs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mUnexpected keyword arguments: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(arg \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m kwargs))\n\u001b[1;32m    248\u001b[0m \u001b[39mif\u001b[39;00m use_reentrant:\n\u001b[0;32m--> 249\u001b[0m     \u001b[39mreturn\u001b[39;00m CheckpointFunction\u001b[39m.\u001b[39;49mapply(function, preserve, \u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    250\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    251\u001b[0m     \u001b[39mreturn\u001b[39;00m _checkpoint_without_reentrant(\n\u001b[1;32m    252\u001b[0m         function,\n\u001b[1;32m    253\u001b[0m         preserve,\n\u001b[1;32m    254\u001b[0m         \u001b[39m*\u001b[39margs,\n\u001b[1;32m    255\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    256\u001b[0m     )\n","File \u001b[0;32m/srv/data/joeh97/anaconda3/envs/uni-morty/lib/python3.11/site-packages/torch/autograd/function.py:506\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    504\u001b[0m     \u001b[39m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    505\u001b[0m     args \u001b[39m=\u001b[39m _functorch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 506\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mapply(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39msetup_context \u001b[39m==\u001b[39m _SingleLevelFunction\u001b[39m.\u001b[39msetup_context:\n\u001b[1;32m    509\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    510\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    511\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    512\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mstaticmethod. For more details, please see \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    513\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[39m'\u001b[39m)\n","File \u001b[0;32m/srv/data/joeh97/anaconda3/envs/uni-morty/lib/python3.11/site-packages/torch/utils/checkpoint.py:107\u001b[0m, in \u001b[0;36mCheckpointFunction.forward\u001b[0;34m(ctx, run_function, preserve_rng_state, *args)\u001b[0m\n\u001b[1;32m    104\u001b[0m ctx\u001b[39m.\u001b[39msave_for_backward(\u001b[39m*\u001b[39mtensor_inputs)\n\u001b[1;32m    106\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 107\u001b[0m     outputs \u001b[39m=\u001b[39m run_function(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    108\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n","File \u001b[0;32m/srv/data/joeh97/anaconda3/envs/uni-morty/lib/python3.11/site-packages/torch/utils/checkpoint.py:318\u001b[0m, in \u001b[0;36mcheckpoint_sequential.<locals>.run_function.<locals>.forward\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39minput\u001b[39m):\n\u001b[1;32m    317\u001b[0m     \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(start, end \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m--> 318\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m functions[j](\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    319\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n","File \u001b[0;32m/srv/data/joeh97/anaconda3/envs/uni-morty/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m/srv/data/joeh97/anaconda3/envs/uni-morty/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[39mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[39mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m    172\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[1;32m    173\u001b[0m     \u001b[39m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_mean\n\u001b[1;32m    175\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats\n\u001b[1;32m    176\u001b[0m     \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    177\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_var \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    178\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m    179\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m    180\u001b[0m     bn_training,\n\u001b[1;32m    181\u001b[0m     exponential_average_factor,\n\u001b[1;32m    182\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps,\n\u001b[1;32m    183\u001b[0m )\n","File \u001b[0;32m/srv/data/joeh97/anaconda3/envs/uni-morty/lib/python3.11/site-packages/torch/nn/functional.py:2450\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2447\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[1;32m   2448\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[0;32m-> 2450\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m   2451\u001b[0m     \u001b[39minput\u001b[39;49m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled\n\u001b[1;32m   2452\u001b[0m )\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.51 GiB (GPU 0; 11.91 GiB total capacity; 1.58 GiB already allocated; 802.00 MiB free; 1.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}],"source":["run_wdd(\n","    epochs=1,\n","    num_workers=4,\n","    continue_training=False,\n","    gt_data_path=    \"../../../data/wdd_ground_truth/ground_truth_wdd_angles.pickle\",\n","    remap_wdd_dir=      \"../../../data/wdd_ground_truth/\" ,\n","    checkpoint_path= \"./wdd_filtering_supervised_model.pt\",\n","    images_in_archives=True,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0dc4453a7a0f439aa97bfa86ac94f464":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"13ac7b6f0f4a4e65962f14e14be9c9a3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"45001d3bcd7345ef97da5067ffda7329":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5973c46b83b14118a0d73c52f49467a6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ea8f07dc73514c3ca54a31221587325c","IPY_MODEL_ab59a22a2f894e18a79c5a5418003716","IPY_MODEL_dfdd2fb7295b46b2842a79c9340fd28e"],"layout":"IPY_MODEL_0dc4453a7a0f439aa97bfa86ac94f464"}},"693f4e338ac94244afdebff738d77bd1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"77b87fb1739741b7849f2984d7afdc5c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7bca261d5a534ed1805f89985975c0e0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ab59a22a2f894e18a79c5a5418003716":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_13ac7b6f0f4a4e65962f14e14be9c9a3","max":2026,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c33d2c451f434ee6a30f52837b647ff6","value":0}},"c33d2c451f434ee6a30f52837b647ff6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dfdd2fb7295b46b2842a79c9340fd28e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_45001d3bcd7345ef97da5067ffda7329","placeholder":"​","style":"IPY_MODEL_77b87fb1739741b7849f2984d7afdc5c","value":" 0/2026 [02:14&lt;?, ?it/s]"}},"ea8f07dc73514c3ca54a31221587325c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7bca261d5a534ed1805f89985975c0e0","placeholder":"​","style":"IPY_MODEL_693f4e338ac94244afdebff738d77bd1","value":"  0%"}}}}},"nbformat":4,"nbformat_minor":0}
