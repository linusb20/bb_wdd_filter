{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"y415xgE_VkV2"},"source":["# What is this Notebook?"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"qvjIJhXqVdo6"},"source":["Hallo Leute,\n","\n","das ist der Versuch das Conv. Neuronale Netz (CNN) vom *Filter Network* des *Waggle Dance Detectors* zum Laufen zu bringen.\n","\n","**Source code**: [GitHub: BioroboticsLab/bb_wdd_filter](https://github.com/BioroboticsLab/bb_wdd_filter)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"cogE5qn7SpZy"},"source":["# Implementation 02 - Clone from GitHub"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"qbXcCCYGUssR"},"source":["### Train Model"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33261,"status":"ok","timestamp":1683540080651,"user":{"displayName":"IT Joe","userId":"14568903722424047290"},"user_tz":-120},"id":"GMDHkoA-m61R","outputId":"03f64dd4-e3de-4f0b-cf51-c3285e64f0c0"},"outputs":[{"name":"stderr","output_type":"stream","text":["/srv/data/joeh97/anaconda3/envs/uni-morty/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["#%pip install git+https://github.com/linusb20/bb_wdd_filter.git\n","import bb_wdd_filter\n","import wandb"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2156,"status":"ok","timestamp":1683542176104,"user":{"displayName":"IT Joe","userId":"14568903722424047290"},"user_tz":-120},"id":"vcri3xrBm-br","outputId":"0fa73637-3659-4ab3-8ab8-78bed91ad6f9"},"outputs":[{"name":"stdout","output_type":"stream","text":["bb-wdd-filter                 0.1        /srv/data/joeh97/github/bb_wdd_filter\n","wandb                         0.15.2\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip list |  grep -E 'bb-wdd-filter|wandb'"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":239,"status":"ok","timestamp":1683541636020,"user":{"displayName":"IT Joe","userId":"14568903722424047290"},"user_tz":-120},"id":"E6WFsITkv50P"},"outputs":[],"source":["import argparse\n","\n","import pickle\n","import numpy as np\n","import os\n","import torch.nn\n","\n","import bb_wdd_filter.dataset\n","import bb_wdd_filter.models_supervised\n","import bb_wdd_filter.trainer_supervised\n","import bb_wdd_filter.visualization\n","\n","\n","def run_wdd(\n","    gt_data_path,\n","    checkpoint_path=None,\n","    continue_training=True,\n","    epochs=1000,\n","    remap_wdd_dir=None,\n","    image_size=32,\n","    images_in_archives=True,\n","    multi_gpu=False,\n","    image_scale=0.5,\n","    batch_size=\"auto\",\n","    max_lr=0.002 * 8,\n","    wandb_entity=None,\n","    wandb_project=\"wdd-image-classification\",\n","):\n","    \"\"\"\n","    Arguments:\n","        gt_data_path (string)\n","            Path to the .pickle file containing the ground-truth labels and paths.\n","        remap_wdd_dir (string, optional)\n","            Prefix of the path where the image data is saved. The paths in gt_data_path\n","            will be changed to point to this directory instead.\n","        images_in_archives (bool)\n","            Whether the images of the single waggle frames are saved withing an images.zip\n","            file in each WDD subdirectory.\n","        checkpoint_path (string, optional)\n","            Filename to which the model will be saved regularly during training.\n","            The model will be saved on every epoch AND every X batches.\n","        continue_training (bool)\n","            Whether to try to continue training from last checkpoint. Will use the same\n","            wandb run ID. Auto set to \"false\" in case no checkpoint is found.\n","        epochs (int)\n","            Number of epochs to train for.\n","            As the model is saved after every epoch in 'checkpoint_path' and as the logs are\n","            streamed live to wandb.ai, it's save to interrupt the training after any epoch.\n","        image_size (int)\n","            Width and height of images that are passed to the model.\n","        image_scale (float)\n","            Scale factor for the data. E.g. 0.5 will scale the images to half resolution.\n","            That allows for a wider FoV for the model by sacrificing some resolution.\n","        max_lr (float)\n","            The training uses a learning rate scheduler (OneCycleLR) for each epoch\n","            where max_lr constitutes the peak learning rate.\n","        wandb_entity (string, optional)\n","            User name for wandb.ai that the training will log data to.\n","        wandb_project (string)\n","            Project name for wandb.ai.\n","\n","    \"\"\"\n","\n","    with open(gt_data_path, \"rb\") as f:\n","        wdd_gt_data = pickle.load(f)\n","        gt_data_df = [(key,) + v for key, v in wdd_gt_data.items()]\n","\n","    all_indices = np.arange(len(gt_data_df))\n","    test_indices = all_indices[::10]\n","    train_indices = [idx for idx in all_indices if not (idx in test_indices)]\n","\n","    print(\"Train set:\")\n","    dataset = bb_wdd_filter.dataset.SupervisedDataset(\n","        [gt_data_df[idx] for idx in train_indices],\n","        images_in_archives=images_in_archives,\n","        image_size=image_size,\n","        load_wdd_vectors=True,\n","        load_wdd_durations=True,\n","        remap_paths_to=remap_wdd_dir,\n","    )\n","\n","    print(\"Test set:\")\n","    # The evaluator's job is to regularly evaluate the training progress on the test dataset.\n","    # It will calculate additional statistics that are logged over the wandb connection.\n","    evaluator = bb_wdd_filter.dataset.SupervisedValidationDatasetEvaluator(\n","        [gt_data_df[idx] for idx in test_indices],\n","        images_in_archives=images_in_archives,\n","        image_size=image_size,\n","        remap_paths_to=remap_wdd_dir,\n","        default_image_scale=image_scale,\n","    )\n","\n","    model = bb_wdd_filter.models_supervised.WDDClassificationModel(\n","        image_size=image_size\n","    )\n","\n","    if multi_gpu:\n","        model = torch.nn.DataParallel(model)\n","\n","    model = model.cuda()\n","\n","    if batch_size == \"auto\":\n","        # The batch size here is calculated so that it fits on two RTX 2080 Ti in multi-GPU mode.\n","        # Note that a smaller batch size might also need a smaller learning rate.\n","        factor = 1\n","        if multi_gpu:\n","            factor = 2\n","        batch_size = int((64 * 7 * factor) / ((image_size * image_size) / (32 * 32)))\n","    else:\n","        batch_size = int(batch_size)\n","\n","    print(\n","        \"N pars: \",\n","        str(sum(p.numel() for p in model.parameters() if p.requires_grad)),\n","        \"batch size: \",\n","        batch_size,\n","    )\n","\n","    wandb_config = None\n","    if False:\n","        # Project name is fixed so far.\n","        # This provides a logging interface to wandb.ai.\n","        wandb_config = (dict(project=wandb_project, entity=wandb_entity),)\n","\n","    trainer = bb_wdd_filter.trainer_supervised.SupervisedTrainer(\n","        dataset,\n","        model,\n","        wandb_config=dict(),\n","        save_path=checkpoint_path,\n","        batch_size=batch_size,\n","        num_workers=0,\n","        continue_training=continue_training,\n","        image_size=image_size,\n","        batch_sampler_kwargs=dict(\n","            image_scale_factor=image_scale,\n","            inflate_dataset_factor=1000,\n","            augmentation_per_image=False,\n","        ),\n","        test_set_evaluator=evaluator,\n","        eval_test_set_every_n_samples=2000,\n","        save_every_n_samples=200000,\n","        max_lr=max_lr,\n","        batches_to_reach_maximum_augmentation=1000,\n","    )\n","\n","    trainer.run_epochs(epochs)\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":890},"executionInfo":{"elapsed":452459,"status":"error","timestamp":1683542831876,"user":{"displayName":"IT Joe","userId":"14568903722424047290"},"user_tz":-120},"id":"YN6diVK28c6L","outputId":"bc549ae8-8552-4557-977c-fe0b1e0114c3"},"outputs":[],"source":["#import wandb\n","#wandb.init()"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":684,"referenced_widgets":["5973c46b83b14118a0d73c52f49467a6","ea8f07dc73514c3ca54a31221587325c","ab59a22a2f894e18a79c5a5418003716","dfdd2fb7295b46b2842a79c9340fd28e","0dc4453a7a0f439aa97bfa86ac94f464","7bca261d5a534ed1805f89985975c0e0","693f4e338ac94244afdebff738d77bd1","13ac7b6f0f4a4e65962f14e14be9c9a3","c33d2c451f434ee6a30f52837b647ff6","45001d3bcd7345ef97da5067ffda7329","77b87fb1739741b7849f2984d7afdc5c"]},"executionInfo":{"elapsed":134810,"status":"error","timestamp":1683542321997,"user":{"displayName":"IT Joe","userId":"14568903722424047290"},"user_tz":-120},"id":"e7vTFG5lxbEZ","outputId":"9e21938a-29be-4cf8-bad6-c27b6d72d210"},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: './wdd_ground_truth/wdd_ground_truth/ground_truth_wdd_angles.pickle'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m run_wdd(\n\u001b[1;32m      2\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m      3\u001b[0m     continue_training\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m      4\u001b[0m     gt_data_path\u001b[39m=\u001b[39;49m    \u001b[39m\"\u001b[39;49m\u001b[39m./wdd_ground_truth/wdd_ground_truth/ground_truth_wdd_angles.pickle\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      5\u001b[0m     checkpoint_path\u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m./wdd_ground_truth/wdd_ground_truth/wdd_filtering_supervised_model.pt\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      6\u001b[0m     remap_wdd_dir\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m./wdd_ground_truth/wdd_ground_truth/wdd_ground_truth/\u001b[39;49m\u001b[39m\"\u001b[39;49m ,\n\u001b[1;32m      7\u001b[0m     images_in_archives\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m      8\u001b[0m )\n","Cell \u001b[0;32mIn[4], line 64\u001b[0m, in \u001b[0;36mrun_wdd\u001b[0;34m(gt_data_path, checkpoint_path, continue_training, epochs, remap_wdd_dir, image_size, images_in_archives, multi_gpu, image_scale, batch_size, max_lr, wandb_entity, wandb_project)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_wdd\u001b[39m(\n\u001b[1;32m     15\u001b[0m     gt_data_path,\n\u001b[1;32m     16\u001b[0m     checkpoint_path\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m     wandb_project\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwdd-image-classification\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     28\u001b[0m ):\n\u001b[1;32m     29\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[39m    Arguments:\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[39m        gt_data_path (string)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     61\u001b[0m \n\u001b[1;32m     62\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(gt_data_path, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m     65\u001b[0m         wdd_gt_data \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mload(f)\n\u001b[1;32m     66\u001b[0m         gt_data_df \u001b[39m=\u001b[39m [(key,) \u001b[39m+\u001b[39m v \u001b[39mfor\u001b[39;00m key, v \u001b[39min\u001b[39;00m wdd_gt_data\u001b[39m.\u001b[39mitems()]\n","File \u001b[0;32m/srv/data/joeh97/anaconda3/envs/uni-morty/lib/python3.11/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './wdd_ground_truth/wdd_ground_truth/ground_truth_wdd_angles.pickle'"]}],"source":["run_wdd(\n","    epochs=1,\n","    continue_training=False,\n","    gt_data_path=    \"  ../../../data/wdd_ground_truth/ground_truth_wdd_angles.pickle\",\n","    remap_wdd_dir=      \"../../../data/wdd_ground_truth/\" ,\n","    checkpoint_path= \"./wdd_filtering_supervised_model.pt\",\n","    images_in_archives=True,\n",")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"SvMR2IgjWpPT"},"source":["# Implementation 01 - Copy & Paste Code into Notebook"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"eLF1x_rXWsAR"},"source":["### Additional Installations"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16974,"status":"ok","timestamp":1683300877867,"user":{"displayName":"Linus Buddrus","userId":"02822583591962403175"},"user_tz":-120},"id":"TmP5L8fbWu1B","outputId":"16d1b09b-87a8-417f-f6c9-0db56ff18dde"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting madgrad\n","  Downloading madgrad-1.3.tar.gz (7.9 kB)\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hcanceled\n","\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n","\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (1.2.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (0.56.4)\n","Requirement already satisfied: numpy<1.24,>=1.18 in /usr/local/lib/python3.10/dist-packages (from numba) (1.22.4)\n","Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba) (0.39.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba) (67.7.2)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: imgaug in /usr/local/lib/python3.10/dist-packages (0.4.0)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from imgaug) (4.7.0.72)\n","Requirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (from imgaug) (2.25.1)\n","Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.10/dist-packages (from imgaug) (1.22.4)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from imgaug) (3.7.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from imgaug) (1.10.1)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from imgaug) (8.4.0)\n","Requirement already satisfied: Shapely in /usr/local/lib/python3.10/dist-packages (from imgaug) (2.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from imgaug) (1.16.0)\n","Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.10/dist-packages (from imgaug) (0.19.3)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.14.2->imgaug) (23.1)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.14.2->imgaug) (1.4.1)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.14.2->imgaug) (2023.4.12)\n","Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.14.2->imgaug) (3.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug) (2.8.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug) (1.4.4)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug) (1.0.7)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug) (3.0.9)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug) (0.11.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug) (4.39.3)\n"]}],"source":["!pip install madgrad\n","!pip install joblib\n","!pip install numba\n","!pip install imgaug"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"GtUtT1r9SuC1"},"source":["### Visualization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w1GyQum4SwpB"},"outputs":[],"source":["# visualization.py\n","import matplotlib.pyplot as plt\n","import matplotlib.cm\n","import numpy as np\n","import sklearn.decomposition\n","import sklearn.manifold\n","import seaborn as sns\n","import torch\n","import tqdm.auto\n","\n","\n","def sample_embeddings(model, dataset, N=1000, test_batch_size=64, seed=42):\n","    n_batches = N // test_batch_size\n","    state = np.random.get_state()\n","    try:\n","        np.random.seed(seed)\n","        random_samples = list(\n","            np.random.choice(\n","                len(dataset), size=n_batches * test_batch_size, replace=False\n","            )\n","        )\n","    finally:\n","        np.random.set_state(state)\n","    all_embeddings = []\n","    all_indices = []\n","\n","    model.eval()\n","    with torch.no_grad():\n","        for _ in tqdm.auto.tqdm(range(n_batches), total=n_batches, leave=False):\n","            batch_images = []\n","            for _ in range(test_batch_size):\n","                idx = random_samples.pop()\n","                all_indices.append(idx)\n","                images, _ = dataset.__getitem__(\n","                    idx, return_just_one=True, normalize_to_float=True\n","                )\n","                batch_images.append(images)\n","            batch_images = np.stack(batch_images, axis=0)\n","            batch_images = torch.from_numpy(batch_images).cuda()\n","\n","            embeddings = model.embed(batch_images)\n","            all_embeddings.append(embeddings.detach().cpu().numpy())\n","\n","    embeddings = np.concatenate(all_embeddings, axis=0)[:, :, 0, 0]\n","    return embeddings, all_indices\n","\n","\n","def plot_embeddings(\n","    embeddings,\n","    indices,\n","    dataset,\n","    images=None,\n","    labels=None,\n","    scatterplot=False,\n","    display=True,\n","):\n","    embeddings = sklearn.decomposition.PCA(16).fit_transform(embeddings)\n","    embeddings = sklearn.manifold.TSNE(2, init=\"pca\", perplexity=50).fit_transform(\n","        embeddings\n","    )\n","\n","    label_colormap = dict()\n","    if labels is not None:\n","        unique_labels = np.unique(labels)\n","        for idx, label in enumerate(unique_labels):\n","            color = 255.0 * np.array(\n","                matplotlib.cm.tab10(idx / (len(unique_labels) + 1))\n","            )\n","            # Convert to PIL color string..\n","            color = \"rgb({:d}, {:d},{:d})\".format(*list(map(int, color)))\n","            label_colormap[label] = color\n","\n","    from PIL import Image, ImageOps\n","\n","    min_x, max_x = embeddings[:, 0].min(), embeddings[:, 0].max()\n","    min_y, max_y = embeddings[:, 1].min(), embeddings[:, 1].max()\n","\n","    W, H = 4000, 3000\n","    scale_x = W / (max_x - min_x)\n","    scale_y = H / (max_y - min_y)\n","    fig, ax = plt.subplots(figsize=(10, 10))\n","    if not scatterplot:\n","        embedding_image = Image.new(\"RGBA\", (W, H))\n","        for idx, ((x, y), img_idx) in enumerate(zip(embeddings, indices)):\n","            if images is not None:\n","                small = (images[idx] + 1.0) * (255.0 / 2.0)\n","            else:\n","                small = dataset.__getitem__(img_idx, return_just_one=True)[0][0]\n","            small = np.clip(small, 0, 255)\n","\n","            small = Image.fromarray(small.astype(np.uint8))\n","            small = small.resize((128, 128))\n","            small = small.convert(\"RGBA\")\n","\n","            if labels is not None:\n","                small = ImageOps.expand(\n","                    small, border=8, fill=label_colormap[labels[img_idx]]\n","                )\n","            embedding_image.paste(\n","                small, (int((x - min_x) * scale_x), int((y - min_y) * scale_y))\n","            )\n","        ax.imshow(embedding_image)\n","    else:\n","        sns.scatterplot(x=embeddings[:, 0], y=embeddings[:, 1], alpha=0.5)\n","\n","    ax.set_axis_off()\n","\n","    if display:\n","        plt.show()\n","    else:\n","        fig.canvas.draw()\n","        image = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n","        image = image.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n","        plt.close()\n","        return image\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"vtfErDqdSiP3"},"source":["### Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5R5yYNJ-SWws"},"outputs":[],"source":["# dataset.PY\n","from locale import normalize\n","import imgaug.augmenters as iaa\n","import json\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","import pathlib\n","import pandas\n","import pickle\n","import PIL\n","import scipy.spatial.distance\n","import skimage.transform\n","import sklearn.metrics\n","import torchvision.transforms\n","import torch\n","import torch.utils.data\n","import tqdm.auto\n","import zipfile\n","import sklearn.preprocessing\n","\n","class ImageNormalizer:\n","    def __init__(self, image_size, scale_factor):\n","        self.image_size = image_size\n","        self.scale_factor = scale_factor\n","\n","        self.crop = iaa.Sequential(\n","            [\n","                iaa.Resize(scale_factor),\n","                iaa.CenterCropToFixedSize(image_size, image_size),\n","            ]\n","        )\n","\n","        self.normalize_to_float = iaa.Sequential(\n","            [\n","                # Scale to range -1, +1\n","                iaa.Multiply(2.0 / 255.0),\n","                iaa.Add(-1.0),\n","            ]\n","        )\n","\n","    def crop_images(self, images):\n","        images = self.crop.augment_images(images)\n","        return images\n","\n","    def floatify_image(self, img):\n","\n","        if not np.issubdtype(img.dtype, np.floating):\n","            assert img.max() > 1\n","            img = img.astype(np.float32)\n","        else:\n","            img = 255.0 * img\n","\n","        img = self.normalize_to_float.augment_image(img)\n","        return img\n","\n","    def floatify_images(self, images):\n","        images = [self.floatify_image(img) for img in images]\n","        return images\n","\n","    def normalize_images(self, images):\n","        return self.floatify_images(self.crop_images(images))\n","        \n","\n","class WDDDataset:\n","    def __init__(\n","        self,\n","        paths,\n","        temporal_dimension=15,\n","        n_targets=3,\n","        target_offset=2,\n","        images_in_archives=True,\n","        remap_wdd_dir=None,\n","        image_size=128,\n","        silently_skip_invalid=True,\n","        load_wdd_vectors=False,\n","        load_wdd_durations=False,\n","        wdd_angles_for_samples=None,\n","        default_image_scale=0.5,\n","    ):\n","\n","        self.load_wdd_vectors = load_wdd_vectors\n","        self.load_wdd_durations = load_wdd_durations\n","        self.silently_skip_invalid = silently_skip_invalid\n","        self.images_in_archives = images_in_archives\n","        self.sample_gaps = False\n","        self.all_meta_files = []\n","        self.wdd_angles_for_samples = wdd_angles_for_samples\n","\n","        # Count and index waggle information.\n","        if isinstance(paths, str):\n","            if paths.endswith(\".pickle\"):\n","                with open(paths, \"rb\") as f:\n","                    self.all_meta_files = pickle.load(f)[\"json_files\"]\n","            else:\n","                paths = [paths]\n","\n","        if isinstance(paths, list) and str(paths[0]).endswith(\".json\"):\n","            self.all_meta_files += paths\n","        else:\n","            if not self.all_meta_files:\n","                for path in paths:\n","                    self.all_meta_files += list(pathlib.Path(path).glob(\"**/*.json\"))\n","\n","        print(\"Found {} waggle folders.\".format(len(self.all_meta_files)))\n","\n","        if remap_wdd_dir:\n","            for i, path in enumerate(self.all_meta_files):\n","                path = str(path).replace(\"/mnt/thekla/\", remap_wdd_dir)\n","                path = pathlib.Path(path)\n","                self.all_meta_files[i] = path\n","\n","        self.temporal_dimension = temporal_dimension\n","        self.n_targets = n_targets\n","        self.target_offset = target_offset\n","\n","        self.default_normalizer = ImageNormalizer(image_size=image_size,\n","                scale_factor=default_image_scale)\n","        \n","\n","    def load_and_normalize_image(self, filename):\n","        img = WDDDataset.load_image(filename)\n","\n","        img = self.default_normalizer.crop_images(img)\n","        img = self.default_normalizer.floatify_image(img)\n","\n","        return img\n","\n","    @staticmethod\n","    def load_image(filename):\n","        img = PIL.Image.open(filename)\n","        img = np.asarray(img)\n","        assert img.dtype is np.dtype(np.uint8)\n","        return img\n","\n","    @staticmethod\n","    def load_images(filenames, parent=\"\"):\n","        return [WDDDataset.load_image(os.path.join(parent, f)) for f in filenames]\n","\n","    @staticmethod\n","    def load_images_from_archive(filenames, archive):\n","        images = []\n","        for fn in filenames:\n","            with archive.open(fn, \"r\") as f:\n","                images.append(WDDDataset.load_image(f))\n","        return images\n","\n","    @staticmethod\n","    def load_metadata_for_waggle(\n","        waggle_metadata_path,\n","        temporal_dimension,\n","        load_images=True,\n","        images_in_archives=False,\n","        gap_factor=1,\n","        n_targets=0,\n","        target_offset=1,\n","        return_center_images=False,\n","    ):\n","\n","        waggle_dir = waggle_metadata_path.parent\n","\n","        with open(waggle_metadata_path, \"r\") as f:\n","            waggle_metadata = json.load(f)\n","\n","        available_frames_length = len(waggle_metadata[\"frame_timestamps\"])\n","        try:\n","            waggle_angle = waggle_metadata[\"waggle_angle\"]\n","            assert np.abs(waggle_angle) < np.pi * 2.0\n","            waggle_duration = waggle_metadata[\"waggle_duration\"]\n","        except:\n","            waggle_angle = np.nan\n","            waggle_duration = np.nan\n","\n","        if temporal_dimension is not None:\n","            target_sequence_length = n_targets * target_offset\n","            sequence_length = int(\n","                gap_factor * temporal_dimension + target_sequence_length\n","            )\n","\n","            if not return_center_images:\n","                sequence_start = np.random.randint(\n","                    0, available_frames_length - sequence_length\n","                )\n","            else:\n","                sequence_center = available_frames_length // 2\n","                sequence_start = sequence_center - sequence_length // 2\n","\n","            assert available_frames_length >= target_sequence_length + sequence_length\n","\n","        def select_images_from_list(images):\n","\n","            if temporal_dimension is None:\n","                if return_center_images:\n","                    n_available_images = len(images)\n","                    if n_available_images > 32:\n","                        images = images[\n","                            (n_available_images // 4) : -(n_available_images // 4)\n","                        ]\n","                return images\n","\n","            if len(images) != available_frames_length:\n","                print(\n","                    \"N images: {}, available_frames_length: {}\".format(\n","                        len(images), available_frames_length\n","                    )\n","                )\n","\n","            assert len(images) == available_frames_length\n","            images = images[sequence_start : (sequence_start + sequence_length)]\n","\n","            targets_start = sequence_length - target_sequence_length\n","\n","            if n_targets != 0:\n","                targets = images[targets_start:][::target_offset]\n","            else:\n","                targets = []\n","\n","            if temporal_dimension == sequence_length - target_sequence_length:\n","                images = images[:targets_start]\n","            else:\n","                if return_center_images:\n","                    mid = len(images) // 2\n","                    margin = temporal_dimension // 2\n","                    images = images[(mid - margin) : (mid + margin + 1)]\n","                else:\n","                    images = [\n","                        images[idx]\n","                        for idx in sorted(\n","                            np.random.choice(\n","                                sequence_length - target_sequence_length,\n","                                size=temporal_dimension,\n","                                replace=False,\n","                            )\n","                        )\n","                    ]\n","            return images + targets\n","\n","        if images_in_archives:\n","            zip_file_path = os.path.join(waggle_dir, \"images.zip\")\n","            if not os.path.exists(zip_file_path):\n","                print(\"{} does not exist.\".format(zip_file_path))\n","                return None, None\n","\n","            try:\n","                with zipfile.ZipFile(zip_file_path, \"r\") as zf:\n","                    images = list(sorted(zf.namelist()))\n","                    images = select_images_from_list(images)\n","\n","                    if load_images:\n","                        images = WDDDataset.load_images_from_archive(images, zf)\n","            except zipfile.BadZipFile:\n","                print(\"ZipFile corrupt: {}\".format(zip_file_path))\n","                return None, None\n","\n","        else:\n","            images = list(\n","                sorted([f for f in os.listdir(waggle_dir) if f.endswith(\"png\")])\n","            )\n","            if len(images) == 0:\n","                print(\"No images found in folder {}.\".format(waggle_dir))\n","            assert len(images) > 0\n","\n","            images = select_images_from_list(images)\n","            if load_images:\n","                images = WDDDataset.load_images(images, waggle_dir)\n","\n","        return images, waggle_angle, waggle_duration\n","\n","    def __len__(self):\n","        return len(self.all_meta_files)\n","\n","    def __getitem__(\n","        self,\n","        i,\n","        aug=None,\n","        return_just_one=False,\n","        normalize_to_float=False,\n","        return_center_images=False,\n","    ):\n","        waggle_metadata_path = self.all_meta_files[i]\n","\n","        images, waggle_angle, waggle_duration = WDDDataset.load_metadata_for_waggle(\n","            waggle_metadata_path,\n","            self.temporal_dimension,\n","            images_in_archives=self.images_in_archives,\n","            n_targets=self.n_targets,\n","            target_offset=self.target_offset,\n","            return_center_images=return_center_images,\n","        )\n","\n","        if self.wdd_angles_for_samples is not None:\n","            waggle_angle = self.wdd_angles_for_samples[i]\n","\n","        if images is None:\n","            if self.silently_skip_invalid:\n","                return self[i + 1]\n","            else:\n","                return None, None, None\n","        if return_just_one:\n","            images = images[:1]\n","        # images = WDDDataset.load_images(image_filenames, parent=waggle_metadata_path.parent)\n","        if aug is not None:\n","            images, waggle_angle = aug(images, waggle_angle)\n","        else:\n","            images = self.default_normalizer.crop_images(images)\n","\n","        if normalize_to_float:\n","            images = self.default_normalizer.floatify_images(images)\n","\n","        images = np.stack(images, axis=0)  # Stack over channels.\n","\n","        if self.load_wdd_vectors:\n","            waggle_vector = np.zeros(shape=(2,), dtype=np.float32)\n","            if np.isfinite(waggle_angle):\n","                waggle_vector[0] = np.cos(waggle_angle)\n","                waggle_vector[1] = np.sin(waggle_angle)\n","        else:\n","            waggle_vector = None\n","\n","        if not self.load_wdd_durations:\n","            waggle_duration = None\n","\n","        return images, waggle_vector, np.float32(waggle_duration)\n","\n","\n","class BatchSampler:\n","    def __init__(\n","        self,\n","        dataset,\n","        batch_size,\n","        image_size=32,\n","        inflate_dataset_factor=1,\n","        image_scale_factor=0.5,\n","        augmentation_per_image=True,\n","    ):\n","        self.batch_size = batch_size\n","        self.dataset = dataset\n","        self.total_length = len(dataset)\n","        self.inflate_dataset_factor = int(inflate_dataset_factor)\n","        self.image_scale_factor = image_scale_factor\n","        self.augmentation_per_image = augmentation_per_image\n","\n","        self.augmenters = None\n","        self.image_size = image_size\n","\n","    def init_augmenters(self, current_epoch=1, total_epochs=1):\n","\n","        p = np.clip(\n","            0.1 + np.log1p(2 * current_epoch / (max(1, total_epochs - 1))), 0, 1\n","        )\n","\n","        # p = 0.0\n","\n","        # These are applied to each image individually and must not rotate e.g. the images.\n","        self.quality_augmenters = iaa.Sequential(\n","            [\n","                iaa.Sometimes(0.55 * p, iaa.GammaContrast((0.9, 1.1))),\n","                iaa.Sometimes(0.25 * p, iaa.SaltAndPepper(0.01)),\n","                iaa.Sometimes(0.5 * p, iaa.AdditiveGaussianNoise(scale=(0, 0.1))),\n","                iaa.Sometimes(0.25 * p, iaa.GaussianBlur(sigma=(0.0, 0.5))),\n","                iaa.Sometimes(0.25 * p, iaa.Add(value=(-5, 5))),\n","            ]\n","        )\n","        self.rescale = iaa.Sequential(\n","            [\n","                # Scale to range -1, +1\n","                iaa.Multiply(2.0 / 255.0),\n","                iaa.Add(-1.0),\n","            ]\n","        )\n","\n","        # These are sampled for each batch and applied to all images.\n","        self.augmenters = iaa.Sequential(\n","            [\n","                iaa.Affine(\n","                    translate_percent={\"x\": (-0.05, 0.05), \"y\": (-0.05, 0.05)},\n","                    rotate=0.0,\n","                    shear=(-5, 5),\n","                    scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)},\n","                ),\n","                iaa.CropToFixedSize(\n","                    self.image_size * int(1.0 / self.image_scale_factor),\n","                    self.image_size * int(1.0 / self.image_scale_factor),\n","                    position=\"center\",\n","                ),\n","                iaa.Resize(self.image_scale_factor),\n","                iaa.Sometimes(\n","                    0.25 * p,\n","                    iaa.Sequential(\n","                        [\n","                            iaa.Crop(\n","                                percent=(0.1, 0.25),\n","                                sample_independently=False,\n","                                keep_size=False,\n","                            ),\n","                            iaa.PadToFixedSize(\n","                                self.image_size,\n","                                self.image_size,\n","                                position=\"center\",\n","                            ),\n","                        ]\n","                    ),\n","                ),\n","            ]\n","        )\n","\n","        # self.augmenters = iaa.Sequential([iaa.CropToFixedSize(128, 128, position=\"center\")])\n","\n","    def __len__(self):\n","        return (self.total_length * self.inflate_dataset_factor) // self.batch_size\n","\n","    def __getitem__(self, _):\n","\n","        if self.augmenters is None:\n","            self.init_augmenters()\n","\n","        aug = self.augmenters.to_deterministic()\n","\n","        def augment_fn(images, *args):\n","            nonlocal aug\n","            img_aug = self.quality_augmenters\n","            if not self.augmentation_per_image:\n","                # Apply the same augmentation to the whole sequence.\n","                img_aug = img_aug.to_deterministic()\n","            images = img_aug.augment_images(images)\n","            images = self.rescale.augment_images(\n","                [img.astype(np.float32) for img in images]\n","            )\n","            images, angles = BatchSampler.augment_sequence(aug, images, *args)\n","            return images, angles\n","\n","        samples, angles, durations = [], [], []\n","        has_labels = False\n","        labels = []\n","\n","        for _ in range(self.batch_size):\n","            idx = np.random.randint(self.total_length)\n","            sample_data = self.dataset.__getitem__(idx, aug=augment_fn)\n","            label = None\n","            if len(sample_data) == 2:\n","                images, angle, duration = sample_data\n","            else:\n","                images, angle, duration, label = sample_data\n","                has_labels = True\n","\n","            samples.append(images)\n","            angles.append(angle)\n","            durations.append(duration)\n","            labels.append(label)\n","\n","        samples = np.stack(samples, axis=0)\n","        angles = np.stack(angles, axis=0)\n","        durations = np.stack(durations, axis=0)\n","\n","        if not has_labels:\n","            return samples, angles, durations\n","\n","        labels = np.stack(labels, axis=0)\n","        return samples, angles, durations, labels\n","\n","    @classmethod\n","    def augment_sequence(self, aug, images, angle, rotate=True):\n","\n","        rotation = np.random.randint(0, 360)\n","\n","        for idx, img in enumerate(images):\n","            if rotate:\n","                img = skimage.transform.rotate(img, rotation)\n","            images[idx] = aug.augment_image(img)\n","\n","        return images, angle + rotation / 180.0 * np.pi\n","\n","\n","class ValidationDatasetEvaluator:\n","    def __init__(\n","        self,\n","        gt_data_path,\n","        remap_paths_to=\"/mnt/thekla/\",\n","        images_in_archives=False,\n","        image_size=128,\n","        raw_paths=None,\n","        temporal_dimension=None,\n","        return_indices=False,\n","    ):\n","\n","        if raw_paths is None:\n","            self.gt_data_df, paths = ValidationDatasetEvaluator.load_ground_truth_data(\n","                gt_data_path, remap_paths_to=remap_paths_to\n","            )\n","        else:\n","            paths = raw_paths\n","\n","        self.dataset = WDDDataset(\n","            paths,\n","            images_in_archives=images_in_archives,\n","            temporal_dimension=temporal_dimension,\n","            image_size=image_size,\n","            n_targets=0,\n","            silently_skip_invalid=False,\n","        )\n","\n","        self.return_indices = return_indices\n","\n","    @staticmethod\n","    def load_ground_truth_data(gt_data_path, remap_paths_to=\"\"):\n","        if isinstance(gt_data_path, str):\n","            with open(gt_data_path, \"rb\") as f:\n","                wdd_gt_data = pickle.load(f)\n","                gt_data_df = [(key,) + v for key, v in wdd_gt_data.items()]\n","        else:\n","            gt_data_df = gt_data_path\n","\n","        gt_data_df = pandas.DataFrame(\n","            gt_data_df, columns=[\"waggle_id\", \"label\", \"gt_angle\", \"path\"]\n","        )\n","        paths = list(gt_data_df.path.values)\n","\n","        if remap_paths_to:\n","\n","            def rewrite(p):\n","                p = str(p).replace(\"/mnt/curta/storage/beesbook/wdd/\", remap_paths_to)\n","                p = pathlib.Path(p)\n","                return p\n","\n","            paths = [rewrite(p) for p in paths]\n","\n","        return gt_data_df, paths\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, i):\n","        batch_images, vectors, durations = self.dataset.__getitem__(\n","            i, normalize_to_float=True, return_center_images=True\n","        )\n","\n","        if not self.return_indices:\n","            return batch_images\n","        return i, batch_images\n","\n","    def get_images_and_embeddings(\n","        self,\n","        model,\n","        use_last_state=False,\n","        show_progress=True,\n","        get_sample_images=True,\n","        augment_images=False,\n","    ):\n","\n","        augmentations = [None]\n","        if augment_images:\n","            augmentations = [\n","                None,\n","                iaa.Fliplr(1.0),\n","                iaa.Flipud(1.0),\n","                iaa.Sequential([iaa.Fliplr(1.0), iaa.Flipud(1.0)]),\n","            ]\n","\n","        model.eval()\n","        with torch.no_grad():\n","            embeddings = []\n","            sample_images = []\n","            labels = []\n","\n","            trange = range(len(self.dataset))\n","            if show_progress:\n","                trange = tqdm.auto.tqdm(trange)\n","\n","            for i in trange:\n","                original_batch_images = self[i]\n","\n","                for aug in augmentations:\n","                    # Collapse batch dimension.\n","                    batch_images = original_batch_images.copy()\n","\n","                    if aug is not None:\n","                        batch_images = aug.augment_images(batch_images)\n","\n","                    # Add batch dimension.\n","                    batch_images = batch_images[None, :, :, :]\n","\n","                    if get_sample_images:\n","                        temp_dimension = batch_images.shape[0]\n","                        sample_images.append(batch_images[0, temp_dimension // 2])\n","\n","                    batch_images = torch.from_numpy(batch_images).cuda()\n","\n","                    _, embedding = model.embed_sequence(\n","                        batch_images,\n","                        return_full_state=not use_last_state,\n","                        check_length=False,\n","                    )\n","\n","                    if not use_last_state and model.lstm is not None:\n","                        embedding = torch.mean(embedding[:, 0], dim=0)\n","\n","                    embedding = embedding.detach().cpu().numpy().flatten()\n","\n","                    embeddings.append(embedding)\n","                    labels.append(self.gt_data_df.label.iloc[i])\n","\n","        embeddings = np.array(embeddings)\n","\n","        return sample_images, embeddings, labels\n","\n","    def plot_embeddings(self, sample_images, embeddings, labels, **kwargs):\n","\n","        from bb_wdd_filter.visualization import plot_embeddings\n","\n","        return plot_embeddings(\n","            embeddings=embeddings,\n","            indices=np.arange(len(self.dataset)),\n","            dataset=self.dataset,\n","            images=sample_images,\n","            labels=labels,\n","            **kwargs,\n","        )\n","\n","    def calculate_scores(self, embeddings, labels):\n","\n","        import sklearn.linear_model\n","        import sklearn.preprocessing\n","        import sklearn.dummy\n","\n","        unique_labels = list(sorted(np.unique(labels)))\n","        label_encoder = lambda l: np.array([unique_labels.index(x) for x in l])\n","        reg_model = sklearn.linear_model.LogisticRegression()\n","\n","        X = embeddings\n","        Y = label_encoder(labels)\n","\n","        scores = dict()\n","\n","        from sklearn.metrics import make_scorer\n","        import sklearn.metrics\n","\n","        scorers = dict(\n","            accuracy=make_scorer(sklearn.metrics.accuracy_score),\n","            f1=make_scorer(sklearn.metrics.f1_score, average=\"macro\"),\n","            roc_auc_score=make_scorer(\n","                sklearn.metrics.roc_auc_score, multi_class=\"ovr\", needs_proba=True\n","            ),\n","        )\n","\n","        for label in (\"all\", \"waggle\"):\n","            _Y = Y\n","            if label != \"all\":\n","                target_label = unique_labels.index(label)\n","                _Y = (Y == target_label).astype(int)\n","\n","            cv_results = sklearn.model_selection.cross_validate(\n","                reg_model, X, _Y, scoring=scorers, cv=10\n","            )\n","\n","            for metric_name, metric_results in cv_results.items():\n","                if not metric_name.startswith(\"test_\"):\n","                    continue\n","                scores[f\"{label}_{metric_name}\"] = np.mean(metric_results)\n","\n","        return scores\n","\n","    def evaluate(self, model, embed_kwargs={}, plot_kwargs={}):\n","\n","        images, embeddings, labels = self.get_images_and_embeddings(\n","            model, **embed_kwargs\n","        )\n","        scores = self.calculate_scores(embeddings, labels)\n","        plot = self.plot_embeddings(images, embeddings, labels, **plot_kwargs)\n","\n","        return scores, plot\n","\n","\n","class SupervisedDataset:\n","    def __init__(\n","        self,\n","        gt_paths,\n","        image_size=32,\n","        temporal_dimension=40,\n","        remap_paths_to=\"/mnt/thekla/\",\n","        images_in_archives=False,\n","        **kwargs,\n","    ):\n","\n","        self.gt_data_df, self.paths = ValidationDatasetEvaluator.load_ground_truth_data(\n","            gt_paths, remap_paths_to=remap_paths_to\n","        )\n","        self.dataset = WDDDataset(\n","            self.paths,\n","            images_in_archives=images_in_archives,\n","            temporal_dimension=temporal_dimension,\n","            image_size=image_size,\n","            n_targets=0,\n","            silently_skip_invalid=False,\n","            wdd_angles_for_samples=self.gt_data_df.gt_angle.values,\n","            **kwargs,\n","        )\n","\n","        labels = self.gt_data_df.label.copy()\n","        labels[labels == \"trembling\"] = \"other\"\n","        self.all_labels = [\"other\", \"waggle\", \"ventilating\", \"activating\"]\n","        label_mapper = {s: i for i, s in enumerate(self.all_labels)}\n","        self.Y = np.array([label_mapper[l] for l in labels])\n","\n","    def __len__(self):\n","        return len(self.paths)\n","\n","    def __getitem__(self, i, **kwargs):\n","        images, vector, duration = self.dataset.__getitem__(i, **kwargs)\n","        label = self.Y[i]\n","\n","        # Add empty channel dimension.\n","        images = np.expand_dims(images, 0)\n","\n","        return images, vector, duration, label\n","\n","\n","class SupervisedValidationDatasetEvaluator:\n","    def __init__(\n","        self,\n","        gt_data_path,\n","        remap_paths_to=\"/mnt/thekla/\",\n","        images_in_archives=False,\n","        image_size=128,\n","        temporal_dimension=None,\n","        return_indices=False,\n","        default_image_scale=0.25,\n","        class_labels=[\"other\", \"waggle\", \"ventilating\", \"activating\"]\n","    ):\n","\n","        self.dataset = SupervisedDataset(\n","            gt_data_path,\n","            images_in_archives=images_in_archives,\n","            image_size=image_size,\n","            load_wdd_vectors=True,\n","            load_wdd_durations=True,\n","            remap_paths_to=remap_paths_to,\n","            default_image_scale=default_image_scale,\n","        )\n","\n","        self.return_indices = return_indices\n","        self.class_labels = class_labels\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, i):\n","\n","        item = self.dataset.__getitem__(\n","            i, normalize_to_float=True, return_center_images=True\n","        )\n","\n","        if self.return_indices:\n","            return i, item\n","        return item\n","\n","    def evaluate(self, model, plot_kwargs=dict()):\n","\n","        dataloader = torch.utils.data.DataLoader(\n","            self, num_workers=0, batch_size=16, shuffle=False, drop_last=False\n","        )\n","\n","        all_classes_hat = []\n","        all_vectors_hat = []\n","        all_durations_hat = []\n","        all_classes = self.dataset.Y\n","        all_vectors = []\n","        all_durations = []\n","\n","        for (images, vectors, durations, _) in dataloader:\n","            predictions = model(images.cuda())\n","            assert predictions.shape[2] == 1\n","            assert predictions.shape[3] == 1\n","            assert predictions.shape[4] == 1\n","            predictions = predictions[:, :, 0, 0, 0]\n","\n","            n_classes = 4\n","            classes_hat = predictions[:, :n_classes]\n","            vectors_hat = predictions[:, n_classes : (n_classes + 2)]\n","            durations_hat = predictions[:, (n_classes + 2) : (n_classes + 3)]\n","\n","            vectors_hat = torch.tanh(vectors_hat)\n","            durations_hat = torch.relu(durations_hat)\n","\n","            classes_hat = torch.nn.functional.softmax(classes_hat, dim=1)\n","\n","            classes_hat = classes_hat.detach().cpu().numpy()\n","            vectors_hat = vectors_hat.detach().cpu().numpy()\n","            durations_hat = durations_hat.detach().cpu().numpy()\n","\n","            all_classes_hat.append(classes_hat)\n","            all_vectors_hat.append(vectors_hat)\n","            all_durations_hat.append(durations_hat)\n","            all_vectors.append(vectors)\n","            all_durations.append(durations)\n","\n","        all_classes_hat = np.concatenate(all_classes_hat, axis=0)\n","        all_classes_hat_argmax = np.argmax(all_classes_hat, axis=1)\n","        all_vectors_hat = np.concatenate(all_vectors_hat, axis=0)\n","        all_durations_hat = np.concatenate(all_durations_hat, axis=0)\n","        all_vectors = np.concatenate(all_vectors, axis=0)\n","        all_durations = np.concatenate(all_durations, axis=0)\n","\n","        metrics = dict()\n","        metrics[\"test_balanced_accuracy\"] = sklearn.metrics.balanced_accuracy_score(\n","            all_classes, all_classes_hat_argmax, adjusted=True\n","        )\n","        try:\n","            metrics[\"test_roc_auc_score\"] = sklearn.metrics.roc_auc_score(\n","                all_classes, all_classes_hat, multi_class=\"ovr\"\n","            )\n","        except ValueError as e:\n","            metrics[\"test_roc_auc_score\"] = np.nan\n","            \n","        metrics[\"test_matthews\"] = sklearn.metrics.matthews_corrcoef(\n","            all_classes, all_classes_hat_argmax\n","        )\n","        metrics[\"test_f1_weighted\"] = sklearn.metrics.f1_score(\n","            all_classes, all_classes_hat_argmax, average=\"weighted\"\n","        )\n","\n","        metrics[\"test_angle_cosine\"] = 1.0 - np.mean(\n","            [\n","                scipy.spatial.distance.cosine(a, b)\n","                for (a, b) in zip(all_vectors, all_vectors_hat)\n","            ]\n","        )\n","\n","        for i in range(1, len(self.class_labels)):\n","            label = self.class_labels[i]\n","            Y_hat = all_classes_hat_argmax == i\n","            Y = all_classes == i\n","\n","            metrics[f\"test_precision_{label}\"] = sklearn.metrics.precision_score(Y, Y_hat)\n","            metrics[f\"test_recall_{label}\"] = sklearn.metrics.recall_score(Y, Y_hat)\n","\n","            \n","\n","        idx = ~pandas.isnull(all_durations)\n","        all_durations = all_durations[idx]\n","        all_durations_hat = all_durations_hat[idx]\n","        metrics[\"test_duration_mse\"] = sklearn.metrics.mean_squared_error(\n","            all_durations, all_durations_hat\n","        )\n","\n","        return metrics\n","\n","\n","class WDDDatasetWithIndicesAndNormalized:\n","    def __init__(self, dataset):\n","        self.dataset = dataset\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, i):\n","        item = self.dataset.__getitem__(\n","            i,\n","            return_just_one=False,\n","            normalize_to_float=True,\n","            return_center_images=True,\n","        )\n","        return i, item\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"fqGuvnEbTAP6"},"source":["### Loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wImAnUcOTA1D"},"outputs":[],"source":["# loss.py\n","\n","import torch\n","\n","\n","def calculate_cpc_loss(encodings, predictions, detach_accuracies=True):\n","\n","    assert isinstance(encodings, list)\n","    assert isinstance(predictions, list)\n","\n","    n_timesteps = len(encodings)\n","    batch_size = encodings[0].shape[0]\n","\n","    nce_loss = 0.0\n","    accuracies = []\n","\n","    for i in range(n_timesteps):\n","        encoding = encodings[i]\n","        prediction = predictions[i]\n","        prediction = torch.swapaxes(prediction, 0, 1)\n","\n","        projections = torch.mm(encoding, prediction)\n","        assert projections.shape[0] == batch_size\n","        assert projections.shape[1] == batch_size\n","\n","        logs_projections = torch.nn.functional.log_softmax(projections, dim=1)\n","\n","        # Count the number of times the highest element is on the diagonal.\n","        hits = logs_projections.argmax(dim=0) == torch.arange(\n","            batch_size, device=logs_projections.device\n","        )\n","        hits = hits.float().mean()\n","        if detach_accuracies:\n","            hits = hits.detach()\n","\n","        accuracies.append(hits)\n","\n","        # Now the InfoNCE loss.\n","        nce = torch.diag(logs_projections).mean()\n","\n","        nce_loss += -1.0 * nce / n_timesteps\n","\n","    losses = {f\"acc_t{i}\": acc for (i, acc) in enumerate(accuracies)}\n","    losses[\"nce_loss\"] = nce_loss\n","\n","    return losses\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"c2ACkpS2TJNT"},"source":["### Models Supervised"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"59xoJTnWTKlf"},"outputs":[],"source":["# models_supervised.py\n","\n","import numpy as np\n","import sklearn.metrics\n","import torch\n","import torch.nn\n","import torch.utils\n","import torchvision.transforms.functional\n","\n","DEFAULT_CLASS_LABELS = [\"other\", \"waggle\", \"ventilating\", \"activating\"]\n","\n","class TensorView(torch.nn.Module):\n","    def __init__(self, *shape):\n","        self.shape = shape\n","\n","    def forward(self, t):\n","        return t.view(*self.shape)\n","\n","\n","class WDDClassificationModel(torch.nn.Module):\n","    def __init__(\n","        self,\n","        n_outputs=7,\n","        temporal_dimension=40,\n","        image_size=32,\n","        scaledown_factor=4,\n","        inplace=False,\n","    ):\n","\n","        super().__init__()\n","\n","        center_stride = image_size // 32\n","        center_padding = 2 if image_size == 32 else 0\n","\n","        if temporal_dimension == 60:\n","            center_temporal_stride = 2\n","            center_temporal_kernel_size = 3\n","        else:\n","            assert temporal_dimension == 40\n","            center_temporal_stride = 1\n","            center_temporal_kernel_size = 5\n","\n","        s = scaledown_factor\n","\n","        self.seq = [\n","            torch.nn.Conv3d(1, 128 // s, kernel_size=5, stride=1, padding=0),\n","            torch.nn.BatchNorm3d(128 // s),\n","            torch.nn.Mish(inplace=inplace),\n","            # 36/56 x 28 - 56 x 60\n","            torch.nn.Conv3d(\n","                128 // s, 64 // s, kernel_size=3, stride=1, padding=0, dilation=2\n","            ),\n","            torch.nn.BatchNorm3d(64 // s),\n","            torch.nn.Mish(inplace=inplace),\n","            # 32/52 x 24 - 52 x 56\n","            torch.nn.Conv3d(\n","                64 // s,\n","                64 // s,\n","                kernel_size=(5, 3, 3),\n","                stride=2,\n","                padding=(3, 1, 1),\n","                dilation=(2, 1, 1),\n","            ),\n","            torch.nn.BatchNorm3d(64 // s),\n","            torch.nn.Mish(inplace=inplace),\n","            # 15/25 x 12 - 25 x 28\n","            torch.nn.Conv3d(\n","                64 // s,\n","                64 // s,\n","                kernel_size=5,\n","                stride=(1, center_stride, center_stride),\n","                padding=(2, center_padding, center_padding),\n","                dilation=1,\n","            ),\n","            torch.nn.BatchNorm3d(64 // s),\n","            torch.nn.Mish(inplace=inplace),\n","            # 15/25 x 12 - 25 x 12\n","            torch.nn.Conv3d(\n","                64 // s,\n","                128 // s,\n","                kernel_size=(center_temporal_kernel_size, 3, 3),\n","                stride=(center_temporal_stride, 2, 2),\n","                padding=(0, 1, 1),\n","                dilation=1,\n","            ),\n","            torch.nn.BatchNorm3d(128 // s),\n","            torch.nn.FeatureAlphaDropout(),\n","            torch.nn.Mish(inplace=inplace),\n","            # 12 x 6 - 12 x 6\n","            torch.nn.Conv3d(\n","                128 // s,\n","                128 // s,\n","                kernel_size=3,\n","                stride=(2, 1, 1),\n","                padding=(0, 1, 1),\n","                dilation=(1, 2, 2),\n","            ),\n","            torch.nn.BatchNorm3d(128 // s),\n","            torch.nn.GLU(dim=1),\n","            torch.nn.Mish(inplace=inplace),\n","            # 5 x 4 - 5 x 4\n","            torch.nn.Conv3d(64 // s, n_outputs, kernel_size=(5, 4, 4)),\n","        ]\n","\n","        self.seq = torch.nn.Sequential(*self.seq)\n","\n","    def postprocess_predictions(self, all_outputs, return_raw=False, as_numpy=False):\n","        \n","        n_classes = 4\n","\n","        classes_hat = all_outputs[:, :n_classes]\n","        vectors_hat = all_outputs[:, n_classes : (n_classes + 2)]\n","        durations_hat = all_outputs[:, (n_classes + 2)]\n","\n","        confidences = None\n","\n","        if not return_raw:\n","            probabilities = torch.nn.functional.softmax(classes_hat, 1)\n","            classes_hat = torch.argmax(probabilities, 1)\n","            confidences = probabilities[np.arange(probabilities.shape[0]), classes_hat]\n","            vectors_hat = torch.tanh(vectors_hat)\n","            durations_hat = torch.relu(durations_hat)\n","        \n","        if as_numpy:\n","            classes_hat = classes_hat.detach().cpu().numpy()\n","            vectors_hat = vectors_hat.detach().cpu().numpy()\n","            durations_hat = durations_hat.detach().cpu().numpy()\n","            if confidences is not None:\n","                confidences = confidences.detach().cpu().numpy()\n","\n","        return classes_hat, vectors_hat, durations_hat, confidences\n","\n","    def forward(self, images):\n","        if self.training:\n","            images.requires_grad = True\n","            output = torch.utils.checkpoint.checkpoint_sequential(self.seq, 4, images)\n","        else:\n","            output = self.seq(images)\n","\n","        if self.training:\n","            shape_correct = (\n","                output.shape[2] == 1 and output.shape[3] == 1 and output.shape[4] == 1\n","            )\n","            if not shape_correct:\n","                raise ValueError(\n","                    \"Incorrect output shape: {} [input shape was {}]\".format(\n","                        output.shape, images.shape\n","                    )\n","                )\n","            output = output[:, :, 0, 0, 0]\n","\n","        return output\n","\n","    def load_state_dict(self, d):\n","        try:\n","            return super().load_state_dict(d)\n","        except Exception as e:\n","            print(\"Failed to load. Trying without DataParallel prefix.\")\n","        # Strip off Wrapper & DataParallel prefix.\n","        d = {key.replace(\"model.module.\", \"\"): v for key, v in d.items()}\n","        return super().load_state_dict(d)\n","\n","\n","# To support DataParallel.\n","class SupervisedModelTrainWrapper(torch.nn.Module):\n","    def __init__(\n","        self, model, class_labels=DEFAULT_CLASS_LABELS\n","    ):\n","\n","        super().__init__()\n","\n","        self.vector_similarity = torch.nn.CosineSimilarity(dim=1)\n","        self.mse = torch.nn.MSELoss()\n","        self.classification_loss = torch.nn.CrossEntropyLoss()\n","        self.class_labels = class_labels\n","\n","        self.model = model\n","    \n","    def forward(self, x):\n","        return self.model(x)\n","\n","    def calc_additional_metrics(\n","        self, predictions, labels, vectors_hat, vectors, durations_hat, durations\n","    ):\n","\n","        import wandb\n","\n","        results = dict()\n","\n","        predictions = torch.nn.functional.softmax(predictions, dim=1)\n","\n","        predictions = predictions.detach().cpu().numpy()\n","        predicted_labels = np.argmax(predictions, axis=1)\n","\n","        labels = labels.detach().cpu().numpy()\n","\n","        results[\"train_conf\"] = wandb.plot.confusion_matrix(\n","            probs=None,\n","            y_true=labels,\n","            preds=predicted_labels,\n","            class_names=self.class_labels,\n","        )\n","\n","        try:\n","            results[\"train_roc_auc\"] = sklearn.metrics.roc_auc_score(\n","                labels,\n","                predictions,\n","                multi_class=\"ovr\",\n","                labels=np.arange(len(self.class_labels)),\n","            )\n","        except:\n","            pass\n","\n","        results[\"train_matthews\"] = sklearn.metrics.matthews_corrcoef(\n","            labels, predicted_labels\n","        )\n","\n","        results[\"train_balanced_accuracy\"] = sklearn.metrics.balanced_accuracy_score(\n","            labels, predicted_labels, adjusted=True\n","        )\n","\n","        results[\"train_f1_weighted\"] = sklearn.metrics.f1_score(\n","            labels, predicted_labels, average=\"weighted\"\n","        )\n","\n","        if vectors_hat is not None:\n","            divergence = self.vector_similarity(vectors_hat, vectors)\n","            results[\"vector_cossim\"] = torch.mean(divergence)\n","\n","            vectors_hat = torch.tanh(vectors_hat)\n","            divergence = self.mse(vectors_hat, vectors)\n","            results[\"vector_mse\"] = torch.mean(divergence)\n","\n","        return results\n","\n","    def run_batch(self, images, vectors, durations, labels):\n","        # print(images.dtype, vectors.dtype, durations.dtype)\n","        batch_size, temp_dimension = images.shape[:2]\n","        model = self.model\n","\n","        all_outputs = model(images)\n","        classes_hat, vectors_hat, durations_hat, _ = model.postprocess_predictions(all_outputs, return_raw=True)        \n","\n","        losses = dict()\n","\n","        losses[\"classification_loss\"] = self.classification_loss(classes_hat, labels)\n","        # losses[\"classification_loss\"] = self.mse(classes_hat, labels)\n","\n","        if vectors is not None:\n","            other_target = 0\n","            valid_indices = labels != other_target\n","\n","            if torch.any(valid_indices):\n","                vectors_hat = vectors_hat[valid_indices]\n","                vectors = vectors[valid_indices]\n","\n","                vectors_hat = torch.tanh(vectors_hat)\n","                # divergence = 1.0 - self.vector_similarity(vectors, vectors_hat)\n","                divergence = self.mse(vectors, vectors_hat)\n","                losses[\"vector_loss\"] = 1.5 * torch.mean(divergence)\n","            else:\n","                vectors_hat = None\n","                vectors = None\n","\n","        if durations is not None:\n","            waggle_target = 1\n","            valid_indices = (labels == waggle_target) & (~torch.isnan(durations))\n","\n","            if torch.any(valid_indices):\n","                durations_hat = durations_hat[valid_indices]\n","                durations = durations[valid_indices]\n","\n","                durations_hat = torch.relu(durations_hat)\n","                divergence = self.mse(durations, durations_hat)\n","                losses[\"duration_loss\"] = 1.0 * torch.mean(divergence)\n","            else:\n","                durations_hat = None\n","                durations = None\n","\n","        with torch.no_grad():\n","            losses[\"additional\"] = self.calc_additional_metrics(\n","                classes_hat, labels, vectors_hat, vectors, durations_hat, durations\n","            )\n","\n","        return losses\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ChS6xElbTnl_"},"source":["### Models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BTsxlVmVTK-_"},"outputs":[],"source":["# models.py\n","\n","import numpy as np\n","import torch\n","import torch.nn\n","import torch.utils\n","import torchvision.transforms.functional\n","\n","# from .loss import calculate_cpc_loss\n","\n","\n","class SubsampleBlock(torch.nn.Module):\n","    def __init__(\n","        self, n_channels, n_mid_channels=64, n_out_channels=64, subsample=True\n","    ):\n","        super().__init__()\n","\n","        stride = 1 if not subsample else 2\n","\n","        norm = torch.nn.utils.spectral_norm\n","        self.seq = torch.nn.Sequential(\n","            norm(\n","                torch.nn.Conv2d(\n","                    n_channels, n_mid_channels, kernel_size=3, padding=1, dilation=1\n","                )\n","            ),\n","            torch.nn.GroupNorm(8, n_mid_channels),\n","            torch.nn.GLU(dim=1),\n","            torch.nn.Mish(),\n","            # torch.nn.BatchNorm2d(n_mid_channels // 2),\n","            norm(\n","                torch.nn.Conv2d(\n","                    n_mid_channels // 2,\n","                    n_out_channels,\n","                    kernel_size=3,\n","                    stride=stride,\n","                    padding=1,\n","                )\n","            ),\n","            torch.nn.GroupNorm(8, n_out_channels),\n","            torch.nn.Mish(),\n","            # torch.nn.BatchNorm2d(n_out_channels),\n","        )\n","\n","    def forward(self, x):\n","        return self.seq(x)\n","\n","\n","class EmbeddingModel(torch.nn.Module):\n","    def __init__(self, n_channels=1, temporal_length=15, n_targets=3, image_size=128):\n","        super().__init__()\n","\n","        self.temporal_length = temporal_length\n","        self.n_targets = n_targets\n","\n","        n_mid_channels = 64\n","        norm = torch.nn.utils.spectral_norm\n","\n","        embedding_size = 256\n","        hidden_state_size = 64\n","        f = 2\n","        self.embedding = torch.nn.Sequential(\n","            # 128\n","            SubsampleBlock(\n","                n_channels, 32, 96 // 2 // f, subsample=image_size >= 128\n","            ),  # 64\n","            SubsampleBlock(\n","                96 // 2 // f, 128 // f, 128 // f, subsample=image_size >= 64\n","            ),  # 32\n","            SubsampleBlock(\n","                128 // f, 128 // f, 256 // f, subsample=image_size >= 32\n","            ),  # 16\n","            SubsampleBlock(256 // f, 256 // f, 512 // f),  # 8\n","            SubsampleBlock(512 // f, embedding_size, 2 * embedding_size),  # 4\n","            norm(\n","                torch.nn.Conv2d(\n","                    2 * embedding_size,\n","                    embedding_size,\n","                    kernel_size=4,\n","                )\n","            ),\n","        )\n","\n","        # Input size: b, embedding_size,  temporal_length\n","        if False:\n","            self.lstm = None\n","            self.sequential_embedding = []\n","            current_length = self.temporal_length\n","            current_hidden_size = embedding_size\n","            while current_length >= 4:\n","                self.sequential_embedding += [\n","                    torch.nn.Conv1d(\n","                        current_hidden_size,\n","                        current_hidden_size * 2,\n","                        kernel_size=3,\n","                        stride=1,\n","                    ),\n","                    torch.nn.GroupNorm(8, current_hidden_size * 2),\n","                    torch.nn.Mish(),\n","                ]\n","                current_length -= 2\n","                current_length //= 2\n","\n","                out_size = (\n","                    current_hidden_size * 2\n","                    if current_length >= 4\n","                    else hidden_state_size\n","                )\n","                self.sequential_embedding += [\n","                    torch.nn.Conv1d(\n","                        current_hidden_size * 2,\n","                        out_size,\n","                        kernel_size=3,\n","                        stride=2,\n","                        padding=1,\n","                    ),\n","                    torch.nn.GroupNorm(8, out_size),\n","                    torch.nn.Mish(),\n","                ]\n","\n","                current_hidden_size = out_size\n","\n","            print(current_length)\n","            self.sequential_embedding += [torch.nn.AvgPool1d(current_length)]\n","            self.sequential_embedding = torch.nn.Sequential(*self.sequential_embedding)\n","        else:\n","            self.lstm = torch.nn.LSTM(\n","                input_size=embedding_size,\n","                hidden_size=hidden_state_size,\n","                batch_first=False,\n","            )\n","\n","        self.predictors = torch.nn.ModuleList(\n","            [\n","                torch.nn.Sequential(\n","                    # torch.nn.Linear(hidden_state_size, hidden_state_size // 2),\n","                    # torch.nn.GroupNorm(1, hidden_state_size // 2),\n","                    # torch.nn.Mish(),\n","                    torch.nn.Linear(hidden_state_size, embedding_size),\n","                    # torch.nn.LeakyReLU(),\n","                )\n","                for _ in range(self.n_targets)\n","            ]\n","        )\n","\n","        self.direction_vector_regressor = torch.nn.Sequential(\n","            torch.nn.Linear(hidden_state_size, hidden_state_size * 2),\n","            torch.nn.GroupNorm(8, hidden_state_size * 2),\n","            torch.nn.Mish(),\n","            torch.nn.Linear(hidden_state_size * 2, 2),\n","            torch.nn.Tanh(),\n","        )\n","\n","        self.vector_similarity = torch.nn.CosineSimilarity(dim=1)\n","\n","    def predict_waggle_direction(self, hidden_state):\n","        directions = self.direction_vector_regressor(hidden_state)\n","        lengths = torch.linalg.vector_norm(directions, dim=1) + 1e-3\n","        directions = directions / lengths.unsqueeze(1)\n","        return directions\n","\n","    def embed(self, images):\n","        if self.training:\n","            embedding = torch.utils.checkpoint.checkpoint(self.embedding, images)\n","        else:\n","            embedding = self.embedding(images)\n","        return embedding\n","\n","    def calculate_image_embeddings_for_image_sequences(self, images):\n","\n","        temporal_length = self.temporal_length or images.shape[1]\n","\n","        embeddings = []\n","        for i in range(temporal_length):\n","            e = self.embed(images[:, i : (i + 1), :, :])\n","            embedding_size = e.shape[1]\n","            assert e.shape[2] == 1 and e.shape[3] == 1\n","            e = e[:, :, 0, 0]\n","            embeddings.append(e)\n","\n","        embeddings = torch.stack(embeddings, dim=0)\n","\n","        return embeddings\n","\n","    def embed_sequence(self, images, return_full_state=False, check_length=True):\n","\n","        assert (\n","            (not check_length)\n","            or (self.temporal_length is None)\n","            or (images.shape[1] == self.temporal_length)\n","        )\n","\n","        embeddings = self.calculate_image_embeddings_for_image_sequences(images)\n","\n","        if self.lstm is not None:\n","            out, hidden_states = self.lstm(embeddings)\n","\n","            if not return_full_state:\n","                out = out[-1]  # Last sequence state.\n","\n","        else:\n","            e = torch.transpose(embeddings, 0, 1)\n","            e = torch.transpose(e, 1, 2)\n","\n","            out = self.sequential_embedding(e)\n","\n","            if not return_full_state:\n","                out = out[:, :, -1]\n","\n","        return embeddings, out\n","\n","    def forward(self, images):\n","\n","        image_embeddings, sequential_embeddings = self.embed_sequence(images)\n","        predictions = [\n","            predictor(sequential_embeddings) for predictor in self.predictors\n","        ]\n","\n","        return image_embeddings, sequential_embeddings, predictions\n","\n","    def run_batch(self, images, vectors, durations=None, labels=None):\n","        batch_size, L = images.shape[:2]\n","        base = images[:, : -len(self.predictors), :, :]\n","\n","        assert base.shape[1] == self.temporal_length\n","\n","        target_embeddings = []\n","        targets = images[:, -len(self.predictors) :, :, :]\n","        for idx, predictor in enumerate(self.predictors):\n","\n","            target_embedding = self.embed(targets[:, idx : (idx + 1)])\n","            target_embedding = target_embedding[\n","                :, :, 0, 0\n","            ]  # Collapse dimensions of length 1.\n","            target_embeddings.append(target_embedding)\n","\n","        image_embeddings, sequential_embeddings, predictions = self(base)\n","        losses = calculate_cpc_loss(target_embeddings, predictions)\n","\n","        # Add rotation invariance losses.\n","        angles = np.arange(0, 360 - 45, 15) / 180 * np.pi\n","        angles = np.random.choice(angles, 2, replace=False)\n","\n","        n_angles = angles.shape[0]\n","        rotation_loss = 0.0\n","\n","        for angle in angles:\n","            rotated = torchvision.transforms.functional.rotate(images, angle=angle)\n","            rotated_embeddings = self.calculate_image_embeddings_for_image_sequences(\n","                rotated\n","            )\n","\n","            difference = 1.0 - self.vector_similarity(\n","                image_embeddings, rotated_embeddings\n","            )\n","            # difference = torch.abs(image_embeddings - rotated_embeddings).mean()\n","            rotation_loss += difference.mean()\n","\n","        losses[\"rotation_inv_loss\"] = 100 * rotation_loss / n_angles\n","\n","        if vectors is not None:\n","            valid_indices = ~torch.all(torch.abs(vectors) < 1e-4, dim=1)\n","            if torch.any(valid_indices):\n","                vectors_hat = self.predict_waggle_direction(\n","                    sequential_embeddings[valid_indices]\n","                )\n","                divergence = 1.0 - self.vector_similarity(\n","                    vectors[valid_indices], vectors_hat\n","                )\n","                losses[\"vector_loss\"] = torch.mean(divergence)\n","\n","        return losses\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"64HDvYwHUIAO"},"source":["### Trainer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R8eBO7QfUIUn"},"outputs":[],"source":["### trainer.py\n","\n","import madgrad\n","import numpy as np\n","import pandas\n","import pathlib\n","import shutil\n","import torch\n","import tqdm.auto\n","\n","#from .dataset import BatchSampler\n","#from .visualization import plot_embeddings, sample_embeddings\n","\n","\n","class Trainer:\n","    def __init__(\n","        self,\n","        dataset,\n","        model,\n","        batch_size=32,\n","        use_wandb=None,\n","        wandb_config=dict(),\n","        save_path=\"warn\",\n","        save_every_n_batches=None,\n","        save_every_n_samples=25000,\n","        eval_test_set_every_n_samples=None,\n","        num_workers=16,\n","        continue_training=True,\n","        image_size=128,\n","        test_set_evaluator=None,\n","        batch_sampler_kwargs=dict(),\n","        max_lr=0.001,\n","        batches_to_reach_maximum_augmentation=2000,\n","        run_batch_fn=None,\n","    ):\n","        def init_worker(ID):\n","\n","            import torch\n","            import numpy as np\n","\n","            np.random.seed(torch.initial_seed() % 2 ** 32)\n","\n","            import imgaug\n","\n","            imgaug.seed((torch.initial_seed() + 1) % 2 ** 32)\n","\n","        self.dataset = dataset\n","        self.batch_sampler = BatchSampler(\n","            dataset, batch_size, image_size=image_size, **batch_sampler_kwargs\n","        )\n","        self.dataloader = torch.utils.data.DataLoader(\n","            self.batch_sampler,\n","            num_workers=num_workers,\n","            batch_size=None,\n","            batch_sampler=None,\n","            pin_memory=True,\n","            shuffle=True,\n","            worker_init_fn=init_worker,\n","        )\n","        self.model = model\n","\n","        self.optimizer = madgrad.MADGRAD(self.model.parameters(), lr=0.001)\n","        self.max_lr = max_lr\n","        self.batches_to_reach_maximum_augmentation = (\n","            batches_to_reach_maximum_augmentation\n","        )\n","        self.run_batch_fn = run_batch_fn\n","\n","        self.use_wandb = use_wandb\n","        self.wandb_config = wandb_config\n","        if self.use_wandb is None:\n","            self.use_wandb = len(wandb_config) > 0\n","\n","        if self.use_wandb:\n","            import wandb\n","\n","            self.id = wandb.util.generate_id()\n","            self.wandb_initialized = False\n","        else:\n","            self.id = None\n","\n","        if save_path == \"warn\":\n","            print(\"Warning: No model save path given. Model will not be saved.\")\n","            save_path = None\n","\n","        if save_every_n_batches is None:\n","            save_every_n_batches = save_every_n_samples // batch_size\n","        if eval_test_set_every_n_samples is None:\n","            self.eval_test_set_every_n_batches = save_every_n_batches\n","        else:\n","            self.eval_test_set_every_n_batches = (\n","                eval_test_set_every_n_samples // batch_size\n","            )\n","\n","        self.save_path = save_path\n","        self.save_every_n_batches = save_every_n_batches\n","        self.test_set_evaluator = test_set_evaluator\n","        self.total_batches = 0\n","        self.total_epochs = 0\n","\n","        self.continue_training = continue_training\n","\n","        if continue_training:\n","            self.load_checkpoint()\n","\n","    def run_batch(self, images, vectors, durations=None, labels=None):\n","\n","        current_state = dict()\n","\n","        self.model.train()\n","        images = images.cuda(non_blocking=True)\n","        if vectors is not None and not np.any(pandas.isnull(vectors)):\n","            vectors = vectors.cuda(non_blocking=True)\n","        else:\n","            vectors = None\n","\n","        if durations is not None and not np.all(pandas.isnull(durations)):\n","            durations = durations.cuda(non_blocking=True)\n","        else:\n","            durations = None\n","\n","        if labels is not None:\n","            labels = labels.cuda(non_blocking=True)\n","\n","        self.optimizer.zero_grad()\n","        if self.run_batch_fn is not None:\n","            losses = self.run_batch_fn(self.model, images, vectors, durations, labels)\n","        else:\n","            losses = self.model.run_batch(images, vectors, durations, labels)\n","\n","        total_loss = 0.0\n","        for loss_name, value in losses.items():\n","            if isinstance(value, dict):\n","                current_state = {**current_state, **value}\n","                continue\n","\n","            if value.requires_grad:\n","                total_loss += value\n","\n","            current_state[loss_name] = float(value.detach().cpu().numpy())\n","\n","        total_loss.backward()\n","        self.optimizer.step()\n","\n","        return current_state\n","\n","    def check_init_wandb(self):\n","        if self.use_wandb:\n","            import wandb\n","\n","            if not self.wandb_initialized:\n","                self.wandb_initialized = True\n","                wandb.init(\n","                    id=self.id,\n","                    resume=\"allow\" if self.continue_training else False,\n","                    **self.wandb_config\n","                )\n","\n","                config = wandb.config\n","                config[\"optimizer\"] = type(self.optimizer).__name__\n","\n","    def check_scale_augmenters(self):\n","        if self.total_batches % 100 == 0:\n","            # Scale augmentation.\n","            self.batch_sampler.init_augmenters(\n","                current_epoch=self.total_batches,\n","                total_epochs=self.batches_to_reach_maximum_augmentation,\n","            )\n","\n","    def save_at_n_batches(self):\n","        if self.save_path is not None:\n","            tqdm.auto.tqdm.write(\n","                \"Saving model state at batch {}..\".format(self.total_batches)\n","            )\n","            self.save_state()\n","\n","    def sample_and_save_embedding(self):\n","        import wandb\n","\n","        self.model.eval()\n","\n","        loss_info = dict()\n","\n","        if self.test_set_evaluator is not None:\n","            scores, plot = self.test_set_evaluator.evaluate(\n","                self.model, plot_kwargs=dict(display=False)\n","            )\n","            loss_info = {**loss_info, **scores}\n","            loss_info[\"embedding\"] = wandb.Image(plot)\n","\n","        else:\n","            e, idx = sample_embeddings(self.model, self.dataset)\n","            img = plot_embeddings(\n","                e, idx, self.dataset, scatterplot=False, display=False\n","            )\n","            loss_info[\"embedding\"] = wandb.Image(img)\n","\n","        self.model.train()\n","\n","        return loss_info\n","\n","    def run_epoch(self):\n","\n","        if self.use_wandb:\n","            import wandb\n","\n","        self.check_init_wandb()\n","\n","        n_batches = len(self.batch_sampler)\n","        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n","            self.optimizer, self.max_lr, total_steps=n_batches\n","        )\n","\n","        for _, batch in enumerate(tqdm.auto.tqdm(self.dataloader, leave=False)):\n","\n","            self.check_scale_augmenters()\n","\n","            loss_info = self.run_batch(*batch)\n","\n","            self.total_batches += 1\n","\n","            if (self.total_batches + 1) % (self.save_every_n_batches + 1) == 0:\n","                self.save_at_n_batches()\n","\n","            if (self.total_batches + 1) % (self.eval_test_set_every_n_batches + 1) == 0:\n","                additional_vars = None\n","\n","                if self.use_wandb:\n","                    with torch.no_grad():\n","                        additional_vars = self.sample_and_save_embedding()\n","\n","                if additional_vars is not None:\n","                    loss_info = {**loss_info, **additional_vars}\n","\n","            scheduler.step()\n","\n","            if self.use_wandb:\n","                loss_info[\"learning_rate\"] = scheduler._last_lr\n","                wandb.log(loss_info)\n","\n","    def run_epochs(self, n):\n","        for i in range(n):\n","\n","            self.run_epoch()\n","            self.total_epochs += 1\n","\n","            if self.save_path is not None:\n","                print(\"Saving model state after epoch {}..\".format(i), flush=True)\n","                self.save_state(copy_suffix=\"_epoch{:03d}\".format(self.total_epochs))\n","\n","    def save_state(self, copy_suffix=None):\n","\n","        model_state_dict = self.model.state_dict()\n","\n","        state = dict(\n","            model=model_state_dict,\n","            wandb_id=self.id,\n","            total_batches=self.total_batches,\n","            total_epochs=self.total_epochs,\n","        )\n","        torch.save(state, self.save_path)\n","\n","        if copy_suffix:\n","            ext = pathlib.Path(self.save_path).suffix\n","            copy_path = self.save_path[: -len(ext)] + str(copy_suffix) + ext\n","            shutil.copy(self.save_path, copy_path)\n","\n","    def load_checkpoint(self):\n","        print(\"Loading last checkpoint...\")\n","        state = torch.load(self.save_path)\n","\n","        self.id = state[\"wandb_id\"]\n","        self.total_batches = state[\"total_batches\"]\n","        self.total_epochs = state[\"total_epochs\"]\n","        self.model.load_state_dict(state[\"model\"])\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"QZ_Vq3ueUFi3"},"source":["### Trainer Supervised"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L_4ULlu6T8BY"},"outputs":[],"source":["# trainer_supervised.py\n","import madgrad\n","import numpy as np\n","import pandas\n","import pathlib\n","import shutil\n","import torch\n","import tqdm.auto\n","\n","# from .trainer import Trainer\n","# from .dataset import BatchSampler\n","# from .visualization import plot_embeddings, sample_embeddings\n","# from .models_supervised import SupervisedModelTrainWrapper\n","\n","\n","class SupervisedTrainer(Trainer):\n","    def __init__(self, dataset, model, *args, batch_sampler_kwargs=dict(), **kwargs):\n","\n","        model = SupervisedModelTrainWrapper(model)\n","\n","        super().__init__(\n","            dataset,\n","            model,\n","            *args,\n","            batch_sampler_kwargs={\n","                **dict(inflate_dataset_factor=50),\n","                **batch_sampler_kwargs,\n","            },\n","            **kwargs\n","        )\n","\n","    def sample_and_save_embedding(self):\n","\n","        self.model.eval()\n","\n","        loss_info = dict()\n","\n","        if self.test_set_evaluator is not None:\n","            scores = self.test_set_evaluator.evaluate(\n","                self.model, plot_kwargs=dict(display=False)\n","            )\n","            loss_info = {**loss_info, **scores}\n","\n","        self.model.train()\n","\n","        return loss_info\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":252},"executionInfo":{"elapsed":193,"status":"error","timestamp":1683198912272,"user":{"displayName":"IT Joe","userId":"14568903722424047290"},"user_tz":-120},"id":"up7nYLpaUtt_","outputId":"4fea6587-050a-4eee-9d07-7c3a631d641b"},"outputs":[{"name":"stderr","output_type":"stream","text":["usage: ipykernel_launcher.py [-h] [--index-path INDEX_PATH]\n","                             [--checkpoint-path CHECKPOINT_PATH]\n","                             [--remap-wdd-dir REMAP_WDD_DIR]\n","                             [--continue-training] [--images-in-archives]\n","                             [--multi-gpu] [--epochs EPOCHS]\n","                             [--batch-size BATCH_SIZE] [--max-lr MAX_LR]\n","                             [--wandb-entity WANDB_ENTITY]\n","ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-4bfd9151-c6d2-4797-bcf2-3d9c0e0f0843.json\n"]},{"ename":"SystemExit","evalue":"ignored","output_type":"error","traceback":["An exception has occurred, use %tb to see the full traceback.\n","\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"]}],"source":["from matplotlib import get_data_path\n","### train_model.py\n","\n","import argparse\n","\n","import pickle\n","import numpy as np\n","import os\n","import torch.nn\n","\n","#import bb_wdd_filter.dataset\n","#import bb_wdd_filter.models_supervised\n","#import bb_wdd_filter.trainer_supervised\n","#import bb_wdd_filter.visualization\n","\n","\n","def run(\n","    gt_data_path,\n","    checkpoint_path=None,\n","    continue_training=True,\n","    epochs=1000,\n","    remap_wdd_dir=None,\n","    image_size=32,\n","    images_in_archives=True,\n","    multi_gpu=False,\n","    image_scale=0.5,\n","    batch_size=\"auto\",\n","    max_lr=0.002 * 8,\n","    wandb_entity=None,\n","    wandb_project=\"wdd-image-classification\",\n","):\n","    \"\"\"\n","    Arguments:\n","        gt_data_path (string)\n","            Path to the .pickle file containing the ground-truth labels and paths.\n","        remap_wdd_dir (string, optional)\n","            Prefix of the path where the image data is saved. The paths in gt_data_path\n","            will be changed to point to this directory instead.\n","        images_in_archives (bool)\n","            Whether the images of the single waggle frames are saved withing an images.zip\n","            file in each WDD subdirectory.\n","        checkpoint_path (string, optional)\n","            Filename to which the model will be saved regularly during training.\n","            The model will be saved on every epoch AND every X batches.\n","        continue_training (bool)\n","            Whether to try to continue training from last checkpoint. Will use the same\n","            wandb run ID. Auto set to \"false\" in case no checkpoint is found.\n","        epochs (int)\n","            Number of epochs to train for.\n","            As the model is saved after every epoch in 'checkpoint_path' and as the logs are\n","            streamed live to wandb.ai, it's save to interrupt the training after any epoch.\n","        image_size (int)\n","            Width and height of images that are passed to the model.\n","        image_scale (float)\n","            Scale factor for the data. E.g. 0.5 will scale the images to half resolution.\n","            That allows for a wider FoV for the model by sacrificing some resolution.\n","        max_lr (float)\n","            The training uses a learning rate scheduler (OneCycleLR) for each epoch\n","            where max_lr constitutes the peak learning rate.\n","        wandb_entity (string, optional)\n","            User name for wandb.ai that the training will log data to.\n","        wandb_project (string)\n","            Project name for wandb.ai.\n","\n","    \"\"\"\n","\n","    with open(gt_data_path, \"rb\") as f:\n","        wdd_gt_data = pickle.load(f)\n","        gt_data_df = [(key,) + v for key, v in wdd_gt_data.items()]\n","\n","    all_indices = np.arange(len(gt_data_df))\n","    test_indices = all_indices[::10]\n","    train_indices = [idx for idx in all_indices if not (idx in test_indices)]\n","\n","    print(\"Train set:\")\n","    dataset = SupervisedDataset(\n","        [gt_data_df[idx] for idx in train_indices],\n","        images_in_archives=images_in_archives,\n","        image_size=image_size,\n","        load_wdd_vectors=True,\n","        load_wdd_durations=True,\n","        remap_paths_to=remap_wdd_dir,\n","    )\n","\n","    print(\"Test set:\")\n","    # The evaluator's job is to regularly evaluate the training progress on the test dataset.\n","    # It will calculate additional statistics that are logged over the wandb connection.\n","    evaluator = SupervisedValidationDatasetEvaluator(\n","        [gt_data_df[idx] for idx in test_indices],\n","        images_in_archives=images_in_archives,\n","        image_size=image_size,\n","        remap_paths_to=remap_wdd_dir,\n","        default_image_scale=image_scale,\n","    )\n","\n","    model = WDDClassificationModel(\n","        image_size=image_size\n","    )\n","\n","    if multi_gpu:\n","        model = torch.nn.DataParallel(model)\n","\n","    model = model.cuda()\n","\n","    if batch_size == \"auto\":\n","        # The batch size here is calculated so that it fits on two RTX 2080 Ti in multi-GPU mode.\n","        # Note that a smaller batch size might also need a smaller learning rate.\n","        factor = 1\n","        if multi_gpu:\n","            factor = 2\n","        batch_size = int((64 * 7 * factor) / ((image_size * image_size) / (32 * 32)))\n","    else:\n","        batch_size = int(batch_size)\n","\n","    print(\n","        \"N pars: \",\n","        str(sum(p.numel() for p in model.parameters() if p.requires_grad)),\n","        \"batch size: \",\n","        batch_size,\n","    )\n","\n","    wandb_config = None\n","    if wandb_entity:\n","        # Project name is fixed so far.\n","        # This provides a logging interface to wandb.ai.\n","        wandb_config = (dict(project=wandb_project, entity=wandb_entity),)\n","\n","    trainer = SupervisedTrainer(\n","        dataset,\n","        model,\n","        wandb_config=wandb_config,\n","        save_path=checkpoint_path,\n","        batch_size=batch_size,\n","        num_workers=8,\n","        continue_training=continue_training,\n","        image_size=image_size,\n","        batch_sampler_kwargs=dict(\n","            image_scale_factor=image_scale,\n","            inflate_dataset_factor=1000,\n","            augmentation_per_image=False,\n","        ),\n","        test_set_evaluator=evaluator,\n","        eval_test_set_every_n_samples=2000,\n","        save_every_n_samples=200000,\n","        max_lr=max_lr,\n","        batches_to_reach_maximum_augmentation=1000,\n","    )\n","\n","    trainer.run_epochs(epochs)\n","\n","\n","if __name__ == \"__main__\":\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument(\n","        \"--index-path\",\n","        type=str,\n","        default=\"./ground_truth_wdd_angles.pickle\",\n","    )\n","    parser.add_argument(\n","        \"--checkpoint-path\",\n","        type=str,\n","        default=\"./wdd_filtering_supervised_model.pt\",\n","    )\n","    parser.add_argument(\"--remap-wdd-dir\", type=str, default=\"\")\n","    parser.add_argument(\"--continue-training\", action=\"store_true\")\n","    parser.add_argument(\"--images-in-archives\", action=\"store_true\")\n","    parser.add_argument(\"--multi-gpu\", action=\"store_true\")\n","    parser.add_argument(\"--epochs\", type=int, default=1000)\n","    parser.add_argument(\"--batch-size\", type=float, default=0.002 * 8)\n","    parser.add_argument(\"--max-lr\", default=\"auto\")\n","    parser.add_argument(\"--wandb-entity\", type=str, default=\"d_d\")\n","    args = parser.parse_args()\n","\n","    continue_training = args.continue_training\n","    if continue_training and args.checkpoint_path:\n","        if not os.path.exists(args.checkpoint_path):\n","            print(\"Can not continue training, as no file found at checkpoint location.\")\n","            continue_training = False\n","\n","    run(gt_data_path='./ground_truth_wdd_angles.pickle', epochs=2)\n","\n","    #run(\n","    #    gt_data_path=args.index_path,\n","    #    checkpoint_path=args.checkpoint_path,\n","    #    epochs=args.epochs,\n","    #    continue_training=continue_training,\n","    #    remap_wdd_dir=args.remap_wdd_dir,\n","    #    images_in_archives=args.images_in_archives,\n","    #    multi_gpu=args.multi_gpu,\n","    #    batch_size=args.batch_size,\n","    #    max_lr=args.max_lr,\n","    #    wandb_entity=args.wandb_entity,\n","    #)\n","\n","   # run(\n","   #     # gt_data_path (string) - Path to the .pickle file containing the ground-truth labels and paths.\n","   #     gt_data_path = \"./ground_truth_wdd_angles.pickle\",\n","#\n","   #     # remap_wdd_dir (string, optional) -- Prefix of the path where the image data is saved. The paths in gt_data_path will be changed to point to this directory instead.\n","   #     remap_wdd_dir = \"\",\n","   #     \n","   #     # images_in_archives (bool) - Whether the images of the single waggle frames are saved withing an images.zip file in each WDD subdirectory.\n","   #     images_in_archives = True,\n","#\n","   #     # checkpoint_path (string, optional) - Filename to which the model will be saved regularly during training. The model will be saved on every epoch AND every X batches.\n","   #     \n","   #     \n","   #     # continue_training (bool) - Whether to try to continue training from last checkpoint. Will use the same wandb run ID. Auto set to \"false\" in case no checkpoint is found.\n","   #     continue_training = False,\n","   #     \n","   #     # epochs (int) - Number of epochs to train for. As the model is saved after every epoch in 'checkpoint_path' and as the logs are streamed live to wandb.ai, it's save to interrupt the training after any epoch.\n","   #     epochs = 2,\n","#\n","   #     batch_size = 0.002 * 8,\n","   #     \n","   #     # image_size (int) - Width and height of images that are passed to the model.\n","   #     # image_size = ,\n","   #     \n","   #     # image_scale (float) - Scale factor for the data. E.g. 0.5 will scale the images to half resolution. That allows for a wider FoV for the model by sacrificing some resolution.\n","   #     image_scale = 0.5 ,\n","#\n","   #     # max_lr (float) - The training uses a learning rate scheduler (OneCycleLR) for each epoch where max_lr constitutes the peak learning rate.\n","   #     max_lr = 0.1\n","#\n","   #     # wandb_entity (string, optional) - User name for wandb.ai that the training will log data to.\n","   #     \n","   #     # wandb_project (string) - Project name for wandb.ai.\n","   # )\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c2XQoCTmdRx9"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32852,"status":"ok","timestamp":1683366785569,"user":{"displayName":"Linus Buddrus","userId":"02822583591962403175"},"user_tz":-120},"id":"p-bD8VhZ3EJI","outputId":"f2dbc1c5-a126-4201-a48e-91ecf06dc7da"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://github.com/BioroboticsLab/bb_wdd_filter.git\n","  Cloning https://github.com/BioroboticsLab/bb_wdd_filter.git to /tmp/pip-req-build-zoypgyqp\n","  Running command git clone --filter=blob:none --quiet https://github.com/BioroboticsLab/bb_wdd_filter.git /tmp/pip-req-build-zoypgyqp\n","  Resolved https://github.com/BioroboticsLab/bb_wdd_filter.git to commit 1010371f08ee8608469c60e2ffcb6ba7a463d3c7\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from bb-wdd-filter==0.1) (1.2.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bb-wdd-filter==0.1) (1.22.4)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from bb-wdd-filter==0.1) (1.5.3)\n","Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from bb-wdd-filter==0.1) (0.56.4)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from bb-wdd-filter==0.1) (1.2.2)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bb-wdd-filter==0.1) (2.0.0+cu118)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from bb-wdd-filter==0.1) (0.15.1+cu118)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bb-wdd-filter==0.1) (3.7.1)\n","Collecting pre-commit\n","  Downloading pre_commit-3.3.1-py2.py3-none-any.whl (202 kB)\n","\u001b[2K     \u001b[90m\u001b[0m \u001b[32m202.5/202.5 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting black\n","  Downloading black-23.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n","\u001b[2K     \u001b[90m\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting madgrad\n","  Downloading madgrad-1.3.tar.gz (7.9 kB)\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: imgaug in /usr/local/lib/python3.10/dist-packages (from bb-wdd-filter==0.1) (0.4.0)\n","Collecting mypy-extensions>=0.4.3\n","  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n","Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black->bb-wdd-filter==0.1) (3.3.0)\n","Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black->bb-wdd-filter==0.1) (2.0.1)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from black->bb-wdd-filter==0.1) (8.1.3)\n","Collecting pathspec>=0.9.0\n","  Downloading pathspec-0.11.1-py3-none-any.whl (29 kB)\n","Requirement already satisfied: packaging>=22.0 in /usr/local/lib/python3.10/dist-packages (from black->bb-wdd-filter==0.1) (23.1)\n","Requirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (from imgaug->bb-wdd-filter==0.1) (2.25.1)\n","Requirement already satisfied: Shapely in /usr/local/lib/python3.10/dist-packages (from imgaug->bb-wdd-filter==0.1) (2.0.1)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from imgaug->bb-wdd-filter==0.1) (8.4.0)\n","Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.10/dist-packages (from imgaug->bb-wdd-filter==0.1) (0.19.3)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from imgaug->bb-wdd-filter==0.1) (1.10.1)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from imgaug->bb-wdd-filter==0.1) (4.7.0.72)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from imgaug->bb-wdd-filter==0.1) (1.16.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bb-wdd-filter==0.1) (1.4.4)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bb-wdd-filter==0.1) (4.39.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bb-wdd-filter==0.1) (0.11.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bb-wdd-filter==0.1) (3.0.9)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bb-wdd-filter==0.1) (2.8.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bb-wdd-filter==0.1) (1.0.7)\n","Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->bb-wdd-filter==0.1) (0.39.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba->bb-wdd-filter==0.1) (67.7.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->bb-wdd-filter==0.1) (2022.7.1)\n","Collecting virtualenv>=20.10.0\n","  Downloading virtualenv-20.23.0-py3-none-any.whl (3.3 MB)\n","\u001b[2K     \u001b[90m\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m89.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nodeenv>=0.11.1\n","  Downloading nodeenv-1.7.0-py2.py3-none-any.whl (21 kB)\n","Collecting cfgv>=2.0.0\n","  Downloading cfgv-3.3.1-py2.py3-none-any.whl (7.3 kB)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from pre-commit->bb-wdd-filter==0.1) (6.0)\n","Collecting identify>=1.0.0\n","  Downloading identify-2.5.24-py2.py3-none-any.whl (98 kB)\n","\u001b[2K     \u001b[90m\u001b[0m \u001b[32m98.8/98.8 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->bb-wdd-filter==0.1) (3.1.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bb-wdd-filter==0.1) (3.12.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bb-wdd-filter==0.1) (1.11.1)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->bb-wdd-filter==0.1) (2.0.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bb-wdd-filter==0.1) (3.1.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->bb-wdd-filter==0.1) (4.5.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bb-wdd-filter==0.1) (3.1)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->bb-wdd-filter==0.1) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->bb-wdd-filter==0.1) (16.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->bb-wdd-filter==0.1) (2.27.1)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.14.2->imgaug->bb-wdd-filter==0.1) (2023.4.12)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.14.2->imgaug->bb-wdd-filter==0.1) (1.4.1)\n","Collecting distlib<1,>=0.3.6\n","  Downloading distlib-0.3.6-py2.py3-none-any.whl (468 kB)\n","\u001b[2K     \u001b[90m\u001b[0m \u001b[32m468.5/468.5 kB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bb-wdd-filter==0.1) (2.1.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->bb-wdd-filter==0.1) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->bb-wdd-filter==0.1) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->bb-wdd-filter==0.1) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->bb-wdd-filter==0.1) (2.0.12)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bb-wdd-filter==0.1) (1.3.0)\n","Building wheels for collected packages: bb-wdd-filter, madgrad\n","  Building wheel for bb-wdd-filter (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for bb-wdd-filter: filename=bb_wdd_filter-0.1-py3-none-any.whl size=17785 sha256=9484aab9ab013413fa95f615f3d54fe3b3dd665305a2f2523b4e73791d0b84d7\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-q7f2jqr8/wheels/82/3e/c9/781290929b7ed454a7f72198a4c70ee7bdb038a7b46014f9ec\n","  Building wheel for madgrad (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for madgrad: filename=madgrad-1.3-py3-none-any.whl size=11886 sha256=b4450a005980fda67feabd8bc0863f91527366bd582799045e77a4125f93b55a\n","  Stored in directory: /root/.cache/pip/wheels/d9/a3/83/7ed1ddc517cd87cad4e3a4aec7f8ea1d5e83a5ff282e51490a\n","Successfully built bb-wdd-filter madgrad\n","Installing collected packages: distlib, virtualenv, pathspec, nodeenv, mypy-extensions, madgrad, identify, cfgv, pre-commit, black, bb-wdd-filter\n","Successfully installed bb-wdd-filter-0.1 black-23.3.0 cfgv-3.3.1 distlib-0.3.6 identify-2.5.24 madgrad-1.3 mypy-extensions-1.0.0 nodeenv-1.7.0 pathspec-0.11.1 pre-commit-3.3.1 virtualenv-20.23.0\n"]}],"source":["!pip install git+https://github.com/BioroboticsLab/bb_wdd_filter.git"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2648,"status":"ok","timestamp":1683366657784,"user":{"displayName":"IT Joe","userId":"14568903722424047290"},"user_tz":-120},"id":"cg9oVGSkURWj","outputId":"d872582d-6194-4455-bd64-3120312059b1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Package                       Version\n","----------------------------- --------------------\n","absl-py                       1.4.0\n","alabaster                     0.7.13\n","albumentations                1.2.1\n","altair                        4.2.2\n","anyio                         3.6.2\n","appdirs                       1.4.4\n","argon2-cffi                   21.3.0\n","argon2-cffi-bindings          21.2.0\n","arviz                         0.15.1\n","astropy                       5.2.2\n","astunparse                    1.6.3\n","attrs                         23.1.0\n","audioread                     3.0.0\n","autograd                      1.5\n","Babel                         2.12.1\n","backcall                      0.2.0\n","beautifulsoup4                4.11.2\n","bleach                        6.0.0\n","blis                          0.7.9\n","blosc2                        2.0.0\n","bokeh                         2.4.3\n","branca                        0.6.0\n","CacheControl                  0.12.11\n","cached-property               1.5.2\n","cachetools                    5.3.0\n","catalogue                     2.0.8\n","certifi                       2022.12.7\n","cffi                          1.15.1\n","chardet                       4.0.0\n","charset-normalizer            2.0.12\n","chex                          0.1.7\n","click                         8.1.3\n","cloudpickle                   2.2.1\n","cmake                         3.25.2\n","cmdstanpy                     1.1.0\n","colorcet                      3.0.1\n","colorlover                    0.3.0\n","community                     1.0.0b1\n","confection                    0.0.4\n","cons                          0.4.5\n","contextlib2                   0.6.0.post1\n","contourpy                     1.0.7\n","convertdate                   2.4.0\n","cryptography                  40.0.2\n","cufflinks                     0.17.3\n","cupy-cuda11x                  11.0.0\n","cvxopt                        1.3.0\n","cvxpy                         1.3.1\n","cycler                        0.11.0\n","cymem                         2.0.7\n","Cython                        0.29.34\n","dask                          2022.12.1\n","datascience                   0.17.6\n","db-dtypes                     1.1.1\n","dbus-python                   1.2.16\n","debugpy                       1.6.6\n","decorator                     4.4.2\n","defusedxml                    0.7.1\n","distributed                   2022.12.1\n","dlib                          19.24.1\n","dm-tree                       0.1.8\n","docutils                      0.16\n","dopamine-rl                   4.0.6\n","duckdb                        0.7.1\n","earthengine-api               0.1.350\n","easydict                      1.10\n","ecos                          2.0.12\n","editdistance                  0.6.2\n","en-core-web-sm                3.5.0\n","entrypoints                   0.4\n","ephem                         4.1.4\n","et-xmlfile                    1.1.0\n","etils                         1.2.0\n","etuples                       0.3.8\n","exceptiongroup                1.1.1\n","fastai                        2.7.12\n","fastcore                      1.5.29\n","fastdownload                  0.0.7\n","fastjsonschema                2.16.3\n","fastprogress                  1.0.3\n","fastrlock                     0.8.1\n","filelock                      3.12.0\n","firebase-admin                5.3.0\n","Flask                         2.2.4\n","flatbuffers                   23.3.3\n","flax                          0.6.9\n","folium                        0.14.0\n","fonttools                     4.39.3\n","frozendict                    2.3.7\n","fsspec                        2023.4.0\n","future                        0.18.3\n","gast                          0.4.0\n","GDAL                          3.3.2\n","gdown                         4.6.6\n","gensim                        4.3.1\n","geographiclib                 2.0\n","geopy                         2.3.0\n","gin-config                    0.5.0\n","glob2                         0.7\n","google                        2.0.3\n","google-api-core               2.11.0\n","google-api-python-client      2.84.0\n","google-auth                   2.17.3\n","google-auth-httplib2          0.1.0\n","google-auth-oauthlib          1.0.0\n","google-cloud-bigquery         3.9.0\n","google-cloud-bigquery-storage 2.19.1\n","google-cloud-core             2.3.2\n","google-cloud-datastore        2.15.1\n","google-cloud-firestore        2.11.0\n","google-cloud-language         2.9.1\n","google-cloud-storage          2.8.0\n","google-cloud-translate        3.11.1\n","google-colab                  1.0.0\n","google-crc32c                 1.5.0\n","google-pasta                  0.2.0\n","google-resumable-media        2.5.0\n","googleapis-common-protos      1.59.0\n","googledrivedownloader         0.4\n","graphviz                      0.20.1\n","greenlet                      2.0.2\n","grpcio                        1.54.0\n","grpcio-status                 1.48.2\n","gspread                       3.4.2\n","gspread-dataframe             3.0.8\n","gym                           0.25.2\n","gym-notices                   0.0.8\n","h5netcdf                      1.1.0\n","h5py                          3.8.0\n","hijri-converter               2.3.1\n","holidays                      0.23\n","holoviews                     1.15.4\n","html5lib                      1.1\n","httpimport                    1.3.0\n","httplib2                      0.21.0\n","humanize                      4.6.0\n","hyperopt                      0.2.7\n","idna                          3.4\n","imageio                       2.25.1\n","imageio-ffmpeg                0.4.8\n","imagesize                     1.4.1\n","imbalanced-learn              0.10.1\n","imgaug                        0.4.0\n","importlib-resources           5.12.0\n","imutils                       0.5.4\n","inflect                       6.0.4\n","iniconfig                     2.0.0\n","intel-openmp                  2023.1.0\n","ipykernel                     5.5.6\n","ipython                       7.34.0\n","ipython-genutils              0.2.0\n","ipython-sql                   0.4.1\n","ipywidgets                    7.7.1\n","itsdangerous                  2.1.2\n","jax                           0.4.8\n","jaxlib                        0.4.7+cuda11.cudnn86\n","jieba                         0.42.1\n","Jinja2                        3.1.2\n","joblib                        1.2.0\n","jsonpickle                    3.0.1\n","jsonschema                    4.3.3\n","jupyter-client                6.1.12\n","jupyter-console               6.1.0\n","jupyter_core                  5.3.0\n","jupyter-server                1.24.0\n","jupyterlab-pygments           0.2.2\n","jupyterlab-widgets            3.0.7\n","kaggle                        1.5.13\n","keras                         2.12.0\n","kiwisolver                    1.4.4\n","korean-lunar-calendar         0.3.1\n","langcodes                     3.3.0\n","lazy_loader                   0.2\n","libclang                      16.0.0\n","librosa                       0.10.0.post2\n","lightgbm                      3.3.5\n","lit                           16.0.2\n","llvmlite                      0.39.1\n","locket                        1.0.0\n","logical-unification           0.4.5\n","LunarCalendar                 0.0.9\n","lxml                          4.9.2\n","Markdown                      3.4.3\n","markdown-it-py                2.2.0\n","MarkupSafe                    2.1.2\n","matplotlib                    3.7.1\n","matplotlib-inline             0.1.6\n","matplotlib-venn               0.11.9\n","mdurl                         0.1.2\n","miniKanren                    1.0.3\n","missingno                     0.5.2\n","mistune                       0.8.4\n","mizani                        0.8.1\n","mkl                           2019.0\n","ml-dtypes                     0.1.0\n","mlxtend                       0.14.0\n","more-itertools                9.1.0\n","moviepy                       1.0.3\n","mpmath                        1.3.0\n","msgpack                       1.0.5\n","multipledispatch              0.6.0\n","multitasking                  0.0.11\n","murmurhash                    1.0.9\n","music21                       8.1.0\n","natsort                       8.3.1\n","nbclient                      0.7.4\n","nbconvert                     6.5.4\n","nbformat                      5.8.0\n","nest-asyncio                  1.5.6\n","networkx                      3.1\n","nibabel                       3.0.2\n","nltk                          3.8.1\n","notebook                      6.4.8\n","numba                         0.56.4\n","numexpr                       2.8.4\n","numpy                         1.22.4\n","oauth2client                  4.1.3\n","oauthlib                      3.2.2\n","opencv-contrib-python         4.7.0.72\n","opencv-python                 4.7.0.72\n","opencv-python-headless        4.7.0.72\n","openpyxl                      3.0.10\n","opt-einsum                    3.3.0\n","optax                         0.1.5\n","orbax-checkpoint              0.2.1\n","osqp                          0.6.2.post8\n","packaging                     23.1\n","palettable                    3.3.3\n","pandas                        1.5.3\n","pandas-datareader             0.10.0\n","pandas-gbq                    0.17.9\n","pandocfilters                 1.5.0\n","panel                         0.14.4\n","param                         1.13.0\n","parso                         0.8.3\n","partd                         1.4.0\n","pathlib                       1.0.1\n","pathy                         0.10.1\n","patsy                         0.5.3\n","pep517                        0.13.0\n","pexpect                       4.8.0\n","pickleshare                   0.7.5\n","Pillow                        8.4.0\n","pip                           23.0.1\n","pip-tools                     6.6.2\n","platformdirs                  3.3.0\n","plotly                        5.13.1\n","plotnine                      0.10.1\n","pluggy                        1.0.0\n","polars                        0.17.3\n","pooch                         1.6.0\n","portpicker                    1.3.9\n","prefetch-generator            1.0.3\n","preshed                       3.0.8\n","prettytable                   0.7.2\n","proglog                       0.1.10\n","progressbar2                  4.2.0\n","prometheus-client             0.16.0\n","promise                       2.3\n","prompt-toolkit                3.0.38\n","prophet                       1.1.2\n","proto-plus                    1.22.2\n","protobuf                      3.20.3\n","psutil                        5.9.5\n","psycopg2                      2.9.6\n","ptyprocess                    0.7.0\n","py-cpuinfo                    9.0.0\n","py4j                          0.10.9.7\n","pyarrow                       9.0.0\n","pyasn1                        0.5.0\n","pyasn1-modules                0.3.0\n","pycocotools                   2.0.6\n","pycparser                     2.21\n","pyct                          0.5.0\n","pydantic                      1.10.7\n","pydata-google-auth            1.7.0\n","pydot                         1.4.2\n","pydot-ng                      2.0.0\n","pydotplus                     2.0.2\n","PyDrive                       1.3.1\n","pyerfa                        2.0.0.3\n","pygame                        2.3.0\n","Pygments                      2.14.0\n","PyGObject                     3.36.0\n","pymc                          5.1.2\n","PyMeeus                       0.5.12\n","pymystem3                     0.2.0\n","PyOpenGL                      3.1.6\n","pyparsing                     3.0.9\n","pyrsistent                    0.19.3\n","PySocks                       1.7.1\n","pytensor                      2.10.1\n","pytest                        7.2.2\n","python-apt                    0.0.0\n","python-dateutil               2.8.2\n","python-louvain                0.16\n","python-slugify                8.0.1\n","python-utils                  3.5.2\n","pytz                          2022.7.1\n","pytz-deprecation-shim         0.1.0.post0\n","pyviz-comms                   2.2.1\n","PyWavelets                    1.4.1\n","PyYAML                        6.0\n","pyzmq                         23.2.1\n","qdldl                         0.1.7\n","qudida                        0.0.4\n","regex                         2022.10.31\n","requests                      2.27.1\n","requests-oauthlib             1.3.1\n","requests-unixsocket           0.2.0\n","rich                          13.3.4\n","rpy2                          3.5.5\n","rsa                           4.9\n","scikit-image                  0.19.3\n","scikit-learn                  1.2.2\n","scipy                         1.10.1\n","scs                           3.2.3\n","seaborn                       0.12.2\n","Send2Trash                    1.8.0\n","setuptools                    67.7.2\n","shapely                       2.0.1\n","six                           1.16.0\n","sklearn-pandas                2.2.0\n","smart-open                    6.3.0\n","sniffio                       1.3.0\n","snowballstemmer               2.2.0\n","sortedcontainers              2.4.0\n","soundfile                     0.12.1\n","soupsieve                     2.4.1\n","soxr                          0.3.5\n","spacy                         3.5.2\n","spacy-legacy                  3.0.12\n","spacy-loggers                 1.0.4\n","Sphinx                        3.5.4\n","sphinxcontrib-applehelp       1.0.4\n","sphinxcontrib-devhelp         1.0.2\n","sphinxcontrib-htmlhelp        2.0.1\n","sphinxcontrib-jsmath          1.0.1\n","sphinxcontrib-qthelp          1.0.3\n","sphinxcontrib-serializinghtml 1.1.5\n","SQLAlchemy                    2.0.10\n","sqlparse                      0.4.4\n","srsly                         2.4.6\n","statsmodels                   0.13.5\n","sympy                         1.11.1\n","tables                        3.8.0\n","tabulate                      0.8.10\n","tblib                         1.7.0\n","tenacity                      8.2.2\n","tensorboard                   2.12.2\n","tensorboard-data-server       0.7.0\n","tensorboard-plugin-wit        1.8.1\n","tensorflow                    2.12.0\n","tensorflow-datasets           4.8.3\n","tensorflow-estimator          2.12.0\n","tensorflow-gcs-config         2.12.0\n","tensorflow-hub                0.13.0\n","tensorflow-io-gcs-filesystem  0.32.0\n","tensorflow-metadata           1.13.1\n","tensorflow-probability        0.19.0\n","tensorstore                   0.1.36\n","termcolor                     2.3.0\n","terminado                     0.17.1\n","text-unidecode                1.3\n","textblob                      0.17.1\n","tf-slim                       1.1.0\n","thinc                         8.1.9\n","threadpoolctl                 3.1.0\n","tifffile                      2023.4.12\n","tinycss2                      1.2.1\n","toml                          0.10.2\n","tomli                         2.0.1\n","toolz                         0.12.0\n","torch                         2.0.0+cu118\n","torchaudio                    2.0.1+cu118\n","torchdata                     0.6.0\n","torchsummary                  1.5.1\n","torchtext                     0.15.1\n","torchvision                   0.15.1+cu118\n","tornado                       6.2\n","tqdm                          4.65.0\n","traitlets                     5.7.1\n","triton                        2.0.0\n","tweepy                        4.13.0\n","typer                         0.7.0\n","typing_extensions             4.5.0\n","tzdata                        2023.3\n","tzlocal                       4.3\n","uritemplate                   4.1.1\n","urllib3                       1.26.15\n","vega-datasets                 0.9.0\n","wasabi                        1.1.1\n","wcwidth                       0.2.6\n","webcolors                     1.13\n","webencodings                  0.5.1\n","websocket-client              1.5.1\n","Werkzeug                      2.3.0\n","wheel                         0.40.0\n","widgetsnbextension            3.6.4\n","wordcloud                     1.8.2.2\n","wrapt                         1.14.1\n","xarray                        2022.12.0\n","xarray-einstats               0.5.1\n","xgboost                       1.7.5\n","xlrd                          2.0.1\n","yellowbrick                   1.5\n","yfinance                      0.2.18\n","zict                          3.0.0\n","zipp                          3.15.0\n"]}],"source":["pip list"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0O1DFPCRThYx"},"outputs":[],"source":["# load data from onedrive (ALTERNATIVE: CALL wget so that jupyter downloads it automatically)\n","# remap_wdd_dir = '/content/drive/MyDrive/wdd_ground_truth'\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gSAJwWuD3MNf"},"outputs":[],"source":["from matplotlib import get_data_path\n","### train_model.py\n","\n","import argparse\n","\n","import pickle\n","import numpy as np\n","import os\n","import torch.nn\n","\n","import bb_wdd_filter.dataset\n","import bb_wdd_filter.models_supervised\n","import bb_wdd_filter.trainer_supervised\n","import bb_wdd_filter.visualization\n","\n","\n","def run(\n","    gt_data_path,\n","    checkpoint_path=None,\n","    continue_training=True,\n","    epochs=1000,\n","    remap_wdd_dir=None,\n","    image_size=32,\n","    images_in_archives=True,\n","    multi_gpu=False,\n","    image_scale=0.5,\n","    batch_size=\"auto\",\n","    max_lr=0.002 * 8,\n","    wandb_entity=None,\n","    wandb_project=\"wdd-image-classification\",\n","):\n","    \"\"\"\n","    Arguments:\n","        gt_data_path (string)\n","            Path to the .pickle file containing the ground-truth labels and paths.\n","        remap_wdd_dir (string, optional)\n","            Prefix of the path where the image data is saved. The paths in gt_data_path\n","            will be changed to point to this directory instead.\n","        images_in_archives (bool)\n","            Whether the images of the single waggle frames are saved withing an images.zip\n","            file in each WDD subdirectory.\n","        checkpoint_path (string, optional)\n","            Filename to which the model will be saved regularly during training.\n","            The model will be saved on every epoch AND every X batches.\n","        continue_training (bool)\n","            Whether to try to continue training from last checkpoint. Will use the same\n","            wandb run ID. Auto set to \"false\" in case no checkpoint is found.\n","        epochs (int)\n","            Number of epochs to train for.\n","            As the model is saved after every epoch in 'checkpoint_path' and as the logs are\n","            streamed live to wandb.ai, it's save to interrupt the training after any epoch.\n","        image_size (int)\n","            Width and height of images that are passed to the model.\n","        image_scale (float)\n","            Scale factor for the data. E.g. 0.5 will scale the images to half resolution.\n","            That allows for a wider FoV for the model by sacrificing some resolution.\n","        max_lr (float)\n","            The training uses a learning rate scheduler (OneCycleLR) for each epoch\n","            where max_lr constitutes the peak learning rate.\n","        wandb_entity (string, optional)\n","            User name for wandb.ai that the training will log data to.\n","        wandb_project (string)\n","            Project name for wandb.ai.\n","\n","    \"\"\"\n","\n","    with open(gt_data_path, \"rb\") as f:\n","        wdd_gt_data = pickle.load(f)\n","        gt_data_df = [(key,) + v for key, v in wdd_gt_data.items()]\n","\n","    all_indices = np.arange(len(gt_data_df))\n","    test_indices = all_indices[::10]\n","    train_indices = [idx for idx in all_indices if not (idx in test_indices)]\n","\n","    print(\"Train set:\")\n","    dataset = SupervisedDataset(\n","        [gt_data_df[idx] for idx in train_indices],\n","        images_in_archives=images_in_archives,\n","        image_size=image_size,\n","        load_wdd_vectors=True,\n","        load_wdd_durations=True,\n","        remap_paths_to=remap_wdd_dir,\n","    )\n","\n","    print(\"Test set:\")\n","    # The evaluator's job is to regularly evaluate the training progress on the test dataset.\n","    # It will calculate additional statistics that are logged over the wandb connection.\n","    evaluator = SupervisedValidationDatasetEvaluator(\n","        [gt_data_df[idx] for idx in test_indices],\n","        images_in_archives=images_in_archives,\n","        image_size=image_size,\n","        remap_paths_to=remap_wdd_dir,\n","        default_image_scale=image_scale,\n","    )\n","\n","    model = WDDClassificationModel(\n","        image_size=image_size\n","    )\n","\n","    if multi_gpu:\n","        model = torch.nn.DataParallel(model)\n","\n","    model = model.cuda()\n","\n","    if batch_size == \"auto\":\n","        # The batch size here is calculated so that it fits on two RTX 2080 Ti in multi-GPU mode.\n","        # Note that a smaller batch size might also need a smaller learning rate.\n","        factor = 1\n","        if multi_gpu:\n","            factor = 2\n","        batch_size = int((64 * 7 * factor) / ((image_size * image_size) / (32 * 32)))\n","    else:\n","        batch_size = int(batch_size)\n","\n","    print(\n","        \"N pars: \",\n","        str(sum(p.numel() for p in model.parameters() if p.requires_grad)),\n","        \"batch size: \",\n","        batch_size,\n","    )\n","\n","    wandb_config = None\n","    if wandb_entity:\n","        # Project name is fixed so far.\n","        # This provides a logging interface to wandb.ai.\n","        wandb_config = (dict(project=wandb_project, entity=wandb_entity),)\n","\n","    trainer = SupervisedTrainer(\n","        dataset,\n","        model,\n","        wandb_config=wandb_config,\n","        save_path=checkpoint_path,\n","        batch_size=batch_size,\n","        num_workers=8,\n","        continue_training=continue_training,\n","        image_size=image_size,\n","        batch_sampler_kwargs=dict(\n","            image_scale_factor=image_scale,\n","            inflate_dataset_factor=1000,\n","            augmentation_per_image=False,\n","        ),\n","        test_set_evaluator=evaluator,\n","        eval_test_set_every_n_samples=2000,\n","        save_every_n_samples=200000,\n","        max_lr=max_lr,\n","        batches_to_reach_maximum_augmentation=1000,\n","    )\n","\n","    trainer.run_epochs(epochs)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l0WnSzLN3wy5"},"outputs":[],"source":["# Einfach: Github fork und von da installieren und die prints dort reinhauen \n","# oder\n","# in Colab das Modell tunen\n","\n","run(\n","  gt_data_path=    './ground_truth_wdd_angles.pickle', \n","  epochs=2, \n","  wandb_entity=None,\n","  remap_wdd_dir =  '/content/drive/wdd_ground_truth',\n","  checkpoint_path = '/content/drive/',\n","  images_in_archives = False\n",")\n","\n","\n","    #run(\n","    #    gt_data_path=args.index_path,\n","    #    checkpoint_path=args.checkpoint_path,\n","    #    epochs=args.epochs,\n","    #    continue_training=continue_training,\n","    #    remap_wdd_dir=args.remap_wdd_dir,\n","    #    images_in_archives=args.images_in_archives,\n","    #    multi_gpu=args.multi_gpu,\n","    #    batch_size=args.batch_size,\n","    #    max_lr=args.max_lr,\n","    #    wandb_entity=args.wandb_entity,\n","    #)\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0dc4453a7a0f439aa97bfa86ac94f464":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"13ac7b6f0f4a4e65962f14e14be9c9a3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"45001d3bcd7345ef97da5067ffda7329":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5973c46b83b14118a0d73c52f49467a6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ea8f07dc73514c3ca54a31221587325c","IPY_MODEL_ab59a22a2f894e18a79c5a5418003716","IPY_MODEL_dfdd2fb7295b46b2842a79c9340fd28e"],"layout":"IPY_MODEL_0dc4453a7a0f439aa97bfa86ac94f464"}},"693f4e338ac94244afdebff738d77bd1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"77b87fb1739741b7849f2984d7afdc5c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7bca261d5a534ed1805f89985975c0e0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ab59a22a2f894e18a79c5a5418003716":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_13ac7b6f0f4a4e65962f14e14be9c9a3","max":2026,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c33d2c451f434ee6a30f52837b647ff6","value":0}},"c33d2c451f434ee6a30f52837b647ff6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dfdd2fb7295b46b2842a79c9340fd28e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_45001d3bcd7345ef97da5067ffda7329","placeholder":"","style":"IPY_MODEL_77b87fb1739741b7849f2984d7afdc5c","value":" 0/2026 [02:14&lt;?, ?it/s]"}},"ea8f07dc73514c3ca54a31221587325c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7bca261d5a534ed1805f89985975c0e0","placeholder":"","style":"IPY_MODEL_693f4e338ac94244afdebff738d77bd1","value":"  0%"}}}}},"nbformat":4,"nbformat_minor":0}
